{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import data_io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11823\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>campaign_id</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_stateprefix</th>\n",
       "      <th>location_statefullname</th>\n",
       "      <th>location_city_only</th>\n",
       "      <th>location_tosearch</th>\n",
       "      <th>state_county_fips_str</th>\n",
       "      <th>county_name</th>\n",
       "      <th>search_status</th>\n",
       "      <th>location_lat</th>\n",
       "      <th>location_lng</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5cf2pqo</td>\n",
       "      <td>Dayton, OH</td>\n",
       "      <td>OH</td>\n",
       "      <td>Ohio</td>\n",
       "      <td>Dayton</td>\n",
       "      <td>Dayton, Ohio</td>\n",
       "      <td>39113</td>\n",
       "      <td>Montgomery</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.7589</td>\n",
       "      <td>-84.1919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6wxxj-steve-mcallister</td>\n",
       "      <td>Lilburn, GA</td>\n",
       "      <td>GA</td>\n",
       "      <td>Georgia</td>\n",
       "      <td>Lilburn</td>\n",
       "      <td>Lilburn, Georgia</td>\n",
       "      <td>13135</td>\n",
       "      <td>Gwinnett</td>\n",
       "      <td>NaN</td>\n",
       "      <td>33.8901</td>\n",
       "      <td>-84.1430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HelptheShoeDoctor</td>\n",
       "      <td>San Leandro, CA</td>\n",
       "      <td>CA</td>\n",
       "      <td>California</td>\n",
       "      <td>San Leandro</td>\n",
       "      <td>San Leandro, California</td>\n",
       "      <td>06001</td>\n",
       "      <td>Alameda</td>\n",
       "      <td>NaN</td>\n",
       "      <td>37.7249</td>\n",
       "      <td>-122.1561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>HopeforlifeSharith</td>\n",
       "      <td>Revere, MA</td>\n",
       "      <td>MA</td>\n",
       "      <td>Massachusetts</td>\n",
       "      <td>Revere</td>\n",
       "      <td>Revere, Massachusetts</td>\n",
       "      <td>25025</td>\n",
       "      <td>Suffolk</td>\n",
       "      <td>NaN</td>\n",
       "      <td>42.4084</td>\n",
       "      <td>-71.0120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>LeslieCarey</td>\n",
       "      <td>Wilmington, DE</td>\n",
       "      <td>DE</td>\n",
       "      <td>Delaware</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>Wilmington, Delaware</td>\n",
       "      <td>10003</td>\n",
       "      <td>New Castle</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39.7459</td>\n",
       "      <td>-75.5466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              campaign_id    location_city location_stateprefix  \\\n",
       "0                 5cf2pqo       Dayton, OH                   OH   \n",
       "1  6wxxj-steve-mcallister      Lilburn, GA                   GA   \n",
       "2       HelptheShoeDoctor  San Leandro, CA                   CA   \n",
       "3      HopeforlifeSharith       Revere, MA                   MA   \n",
       "4             LeslieCarey   Wilmington, DE                   DE   \n",
       "\n",
       "  location_statefullname location_city_only        location_tosearch  \\\n",
       "0                   Ohio             Dayton             Dayton, Ohio   \n",
       "1                Georgia            Lilburn         Lilburn, Georgia   \n",
       "2             California        San Leandro  San Leandro, California   \n",
       "3          Massachusetts             Revere    Revere, Massachusetts   \n",
       "4               Delaware         Wilmington     Wilmington, Delaware   \n",
       "\n",
       "  state_county_fips_str county_name search_status  location_lat  location_lng  \n",
       "0                 39113  Montgomery           NaN       39.7589      -84.1919  \n",
       "1                 13135    Gwinnett           NaN       33.8901      -84.1430  \n",
       "2                 06001     Alameda           NaN       37.7249     -122.1561  \n",
       "3                 25025     Suffolk           NaN       42.4084      -71.0120  \n",
       "4                 10003  New Castle           NaN       39.7459      -75.5466  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_locations = pd.read_csv(data_io.input_cleaned/'geolocations'/\n",
    "                            'unique_locations_w_fips_scraped.csv',\n",
    "                              encoding='utf-8',\n",
    "                              dtype={'state_county_fips_str':'str'})\n",
    "unique_locations.dropna(subset=[\"state_county_fips_str\"],inplace=True)\n",
    "print(len(unique_locations))\n",
    "unique_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>last_donation_time</th>\n",
       "      <th>last_update_time</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_country</th>\n",
       "      <th>location_stateprefix</th>\n",
       "      <th>poster</th>\n",
       "      <th>story</th>\n",
       "      <th>title</th>\n",
       "      <th>raised_amnt</th>\n",
       "      <th>...</th>\n",
       "      <th>tag</th>\n",
       "      <th>num_donors</th>\n",
       "      <th>num_likes</th>\n",
       "      <th>num_shares</th>\n",
       "      <th>day</th>\n",
       "      <th>month</th>\n",
       "      <th>year</th>\n",
       "      <th>location_city_only</th>\n",
       "      <th>cancer_in_story</th>\n",
       "      <th>cancer_in_title</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>campaign_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>5cf2pqo</th>\n",
       "      <td>https://www.gofundme.com/f/5cf2pqo</td>\n",
       "      <td>33 mos</td>\n",
       "      <td>2018-07-16T01:05:46-05:00</td>\n",
       "      <td>Dayton, OH</td>\n",
       "      <td>US</td>\n",
       "      <td>OH</td>\n",
       "      <td>Valerie Roof</td>\n",
       "      <td>As many of you already know, Jess recently had...</td>\n",
       "      <td>Help Jimmy Walker, R.I.P. Jess</td>\n",
       "      <td>3500.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Medical, Illness &amp; Healing</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>April</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Dayton</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6wxxj-steve-mcallister</th>\n",
       "      <td>https://www.gofundme.com/f/6wxxj-steve-mcallister</td>\n",
       "      <td>33 mos</td>\n",
       "      <td>2018-10-27T15:04:49-05:00</td>\n",
       "      <td>Lilburn, GA</td>\n",
       "      <td>US</td>\n",
       "      <td>GA</td>\n",
       "      <td>Betty Kinnett</td>\n",
       "      <td>I have a heavy heart today and I need your pra...</td>\n",
       "      <td>Steve McAllister</td>\n",
       "      <td>11009.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Medical, Illness &amp; Healing</td>\n",
       "      <td>98.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>April</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Lilburn</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HelptheShoeDoctor</th>\n",
       "      <td>https://www.gofundme.com/f/HelptheShoeDoctor</td>\n",
       "      <td>28 mos</td>\n",
       "      <td>2020-05-29T23:53:53-05:00</td>\n",
       "      <td>San Leandro, CA</td>\n",
       "      <td>US</td>\n",
       "      <td>CA</td>\n",
       "      <td>Terry O'Neal-Feaster</td>\n",
       "      <td>Allen Feaster (Center) Fought to Close Youth P...</td>\n",
       "      <td>Help Community Activist Beat Cancer</td>\n",
       "      <td>7607.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Medical, Illness &amp; Healing</td>\n",
       "      <td>123.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>April</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>San Leandro</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HopeforlifeSharith</th>\n",
       "      <td>https://www.gofundme.com/f/HopeforlifeSharith</td>\n",
       "      <td>34 mos</td>\n",
       "      <td>2019-03-28T11:16:35-05:00</td>\n",
       "      <td>Revere, MA</td>\n",
       "      <td>US</td>\n",
       "      <td>MA</td>\n",
       "      <td>Felipe Rivas</td>\n",
       "      <td>The girl you see in the picture is my cousin S...</td>\n",
       "      <td>Help Sharith beat Brain Cancer</td>\n",
       "      <td>2024.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Medical, Illness &amp; Healing</td>\n",
       "      <td>53.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>April</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Revere</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LeslieCarey</th>\n",
       "      <td>https://www.gofundme.com/f/LeslieCarey</td>\n",
       "      <td>31 mos</td>\n",
       "      <td>2021-01-28T19:04:16-06:00</td>\n",
       "      <td>Wilmington, DE</td>\n",
       "      <td>US</td>\n",
       "      <td>DE</td>\n",
       "      <td>Susan Oates</td>\n",
       "      <td>Monday, April 2, 2018 ~ After bravely battling...</td>\n",
       "      <td>Help While Healing for Leslie Carey</td>\n",
       "      <td>14825.0</td>\n",
       "      <td>...</td>\n",
       "      <td>Medical, Illness &amp; Healing</td>\n",
       "      <td>182.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.0</td>\n",
       "      <td>April</td>\n",
       "      <td>2018.0</td>\n",
       "      <td>Wilmington</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                      url  \\\n",
       "campaign_id                                                                 \n",
       "5cf2pqo                                https://www.gofundme.com/f/5cf2pqo   \n",
       "6wxxj-steve-mcallister  https://www.gofundme.com/f/6wxxj-steve-mcallister   \n",
       "HelptheShoeDoctor            https://www.gofundme.com/f/HelptheShoeDoctor   \n",
       "HopeforlifeSharith          https://www.gofundme.com/f/HopeforlifeSharith   \n",
       "LeslieCarey                        https://www.gofundme.com/f/LeslieCarey   \n",
       "\n",
       "                       last_donation_time           last_update_time  \\\n",
       "campaign_id                                                            \n",
       "5cf2pqo                            33 mos  2018-07-16T01:05:46-05:00   \n",
       "6wxxj-steve-mcallister             33 mos  2018-10-27T15:04:49-05:00   \n",
       "HelptheShoeDoctor                  28 mos  2020-05-29T23:53:53-05:00   \n",
       "HopeforlifeSharith                 34 mos  2019-03-28T11:16:35-05:00   \n",
       "LeslieCarey                        31 mos  2021-01-28T19:04:16-06:00   \n",
       "\n",
       "                          location_city location_country location_stateprefix  \\\n",
       "campaign_id                                                                     \n",
       "5cf2pqo                      Dayton, OH               US                   OH   \n",
       "6wxxj-steve-mcallister      Lilburn, GA               US                   GA   \n",
       "HelptheShoeDoctor       San Leandro, CA               US                   CA   \n",
       "HopeforlifeSharith           Revere, MA               US                   MA   \n",
       "LeslieCarey              Wilmington, DE               US                   DE   \n",
       "\n",
       "                                      poster  \\\n",
       "campaign_id                                    \n",
       "5cf2pqo                         Valerie Roof   \n",
       "6wxxj-steve-mcallister         Betty Kinnett   \n",
       "HelptheShoeDoctor       Terry O'Neal-Feaster   \n",
       "HopeforlifeSharith              Felipe Rivas   \n",
       "LeslieCarey                      Susan Oates   \n",
       "\n",
       "                                                                    story  \\\n",
       "campaign_id                                                                 \n",
       "5cf2pqo                 As many of you already know, Jess recently had...   \n",
       "6wxxj-steve-mcallister  I have a heavy heart today and I need your pra...   \n",
       "HelptheShoeDoctor       Allen Feaster (Center) Fought to Close Youth P...   \n",
       "HopeforlifeSharith      The girl you see in the picture is my cousin S...   \n",
       "LeslieCarey             Monday, April 2, 2018 ~ After bravely battling...   \n",
       "\n",
       "                                                      title  raised_amnt  ...  \\\n",
       "campaign_id                                                               ...   \n",
       "5cf2pqo                      Help Jimmy Walker, R.I.P. Jess       3500.0  ...   \n",
       "6wxxj-steve-mcallister                     Steve McAllister      11009.0  ...   \n",
       "HelptheShoeDoctor       Help Community Activist Beat Cancer       7607.0  ...   \n",
       "HopeforlifeSharith           Help Sharith beat Brain Cancer       2024.0  ...   \n",
       "LeslieCarey             Help While Healing for Leslie Carey      14825.0  ...   \n",
       "\n",
       "                                               tag num_donors num_likes  \\\n",
       "campaign_id                                                               \n",
       "5cf2pqo                 Medical, Illness & Healing       53.0       NaN   \n",
       "6wxxj-steve-mcallister  Medical, Illness & Healing       98.0       NaN   \n",
       "HelptheShoeDoctor       Medical, Illness & Healing      123.0       NaN   \n",
       "HopeforlifeSharith      Medical, Illness & Healing       53.0       NaN   \n",
       "LeslieCarey             Medical, Illness & Healing      182.0       NaN   \n",
       "\n",
       "                        num_shares  day  month    year location_city_only  \\\n",
       "campaign_id                                                                 \n",
       "5cf2pqo                        NaN  1.0  April  2018.0             Dayton   \n",
       "6wxxj-steve-mcallister         NaN  1.0  April  2018.0            Lilburn   \n",
       "HelptheShoeDoctor              NaN  1.0  April  2018.0        San Leandro   \n",
       "HopeforlifeSharith             NaN  1.0  April  2018.0             Revere   \n",
       "LeslieCarey                    NaN  1.0  April  2018.0         Wilmington   \n",
       "\n",
       "                        cancer_in_story cancer_in_title  \n",
       "campaign_id                                              \n",
       "5cf2pqo                            True           False  \n",
       "6wxxj-steve-mcallister             True           False  \n",
       "HelptheShoeDoctor                  True            True  \n",
       "HopeforlifeSharith                 True            True  \n",
       "LeslieCarey                        True           False  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(data_io.input_cleaned/'gfm'/'US_cancer_campaigns_2018_2021.csv',index_col=[0],\n",
    "                 sep='|',encoding='utf-8')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exclusion_df = pd.read_csv(data_io.input_cleaned/'gfm'/'exclusion_tracker_rd_2.csv',\n",
    "#                               index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drop locations that didn't geocode\n",
    "unique_locations = unique_locations.replace('nan',np.nan).replace('none',np.nan)\n",
    "unique_locations.dropna(subset=['county_name'], inplace=True)\n",
    "\n",
    "county_dict = dict(\n",
    "    zip(unique_locations['location_city'].to_list(), unique_locations['county_name'].to_list()))\n",
    "long_fips_dict = dict(zip(unique_locations['location_city'].to_list(), \n",
    "                          unique_locations['state_county_fips_str'].to_list()))\n",
    "\n",
    "\n",
    "cleaned_location_city = df['location_city']\n",
    "df['location_county'] = cleaned_location_city.map(county_dict)\n",
    "df['location_state_county_fip'] = cleaned_location_city.map(long_fips_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(217, 3)\n"
     ]
    }
   ],
   "source": [
    "unique_locs_to_scrape = df.loc[df.location_state_county_fip.isna(),\n",
    "                               ['location_city','location_city_only','location_stateprefix']].drop_duplicates()\n",
    "print(unique_locs_to_scrape.shape)\n",
    "unique_locs_to_scrape.to_csv(data_io.input_cleaned/'geolocations'/'unique_locations_to_scrape_again.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#exclusion_df.loc['deleted', 'failed_geocode'] = df['county'].isnull().sum()\n",
    "df = df.dropna(subset = ['location_county'])\n",
    "# exclusion_df.loc['total', 'failed_geocode'] = len(df)\n",
    "# exclusion_df.to_csv(data_io.input_cleaned/'gfm'/'final_exclusion_tracker.csv')\n",
    "geo_fail = df[pd.isnull(df['location_county'])]\n",
    "#save failed geocodes to make sure nothing in the US failed\n",
    "\n",
    "save = False\n",
    "if save:\n",
    "    geo_fail.to_csv(data_io.input_cleaned/'gfm'/'master_failed_geocode.csv', encoding='utf-8-sig')\n",
    "df.dropna(subset=['location_county'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define text mining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for additional text mining\n",
    "SEARCH_OPTIONS = pd.read_csv(data_io.gfm/'gfm'/'free_text_search_terms_w_covid.csv')\n",
    "\n",
    "SEARCH_DICT = {k:SEARCH_OPTIONS[k].dropna().to_list() for k in SEARCH_OPTIONS.columns}\n",
    "\n",
    "    \n",
    "def create_dict(search_type):\n",
    "    key_col = 'collapsed_'+search_type\n",
    "    new = SEARCH_OPTIONS.dropna(subset=[search_type])\n",
    "    this_dict = pd.Series(new[key_col].values,index=new[search_type].values).to_dict()\n",
    "    \n",
    "    return this_dict\n",
    "\n",
    "INSURE_DICT = create_dict('insurance_type')\n",
    "#OOP_DICT = create_dict('oop_type')\n",
    "import string   \n",
    "#import regex as re\n",
    "def extract_search_term_regex(x, search_type = 'cancer_type', return_context = False,\n",
    "                             find_uninsured = False, collapse_dict = 'none'):\n",
    "    if type(x) == str:\n",
    "        x = x.lower()\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    search_terms = SEARCH_DICT[search_type]\n",
    "    #match only if char after match is a space or punctuation\n",
    "    if 'cancer' in search_type:\n",
    "        for s in search_terms:\n",
    "            smatch = re.search(s+'\\W', x)\n",
    "            if smatch:\n",
    "                if return_context == True:\n",
    "                    end_smatch = smatch.span()[1]\n",
    "                    new = x[smatch.span()[0]:]\n",
    "                    new = new[0: new.find('.')]\n",
    "                    return new\n",
    "\n",
    "                return(x[smatch.span()[0]:smatch.span()[1]])\n",
    "        return np.nan\n",
    "    else:\n",
    "        return_val = False\n",
    "        uninsure = False\n",
    "        mention = []\n",
    "        collapsed_mention = []\n",
    "        for s in search_terms:\n",
    "            smatch = re.search(s, x)\n",
    "            \n",
    "            if smatch:\n",
    "                if return_context == True:\n",
    "                    \n",
    "                    new = x[smatch.span()[0]:smatch.span()[1]]\n",
    "                    #print(new)\n",
    "                    mention.append(new)\n",
    "                \n",
    "                if find_uninsured == True:\n",
    "                    if INSURE_DICT[s] == 'uninsured' or INSURE_DICT[s] == 'underunisured':\n",
    "                        return True\n",
    "                else:\n",
    "                    return_val = True\n",
    "                \n",
    "                if type(collapse_dict) != str:\n",
    "                    collapsed_mention.append(collapse_dict[s])\n",
    "\n",
    "                return_val = True\n",
    "                \n",
    "        if len(mention) >= 1:\n",
    "            mention = ','.join(mention)\n",
    "        else:\n",
    "            mention = None\n",
    "            \n",
    "        if len(collapsed_mention) >= 1:\n",
    "            collapsed_mention = np.unique(np.asarray(collapsed_mention))\n",
    "            collapsed_mention = list(collapsed_mention)\n",
    "            collapsed_mention = ','.join\n",
    "        else:\n",
    "            collapsed_mention = None\n",
    "            \n",
    "        if type(collapse_dict)!= str:\n",
    "            return collapsed_mention\n",
    "        \n",
    "        return mention if return_context == True else return_val\n",
    "\n",
    "def search_story_and_title(story, title, search_type):\n",
    "    story_truth = extract_search_term_regex(story, search_type = search_type)\n",
    "    title_truth = extract_search_term_regex(title, search_type = search_type)\n",
    "    if title_truth == True or story_truth == True:\n",
    "        return True\n",
    "    elif title_truth == False and story_truth == False:\n",
    "        return False\n",
    "\n",
    "def get_all_mentions(story, title, search_type):\n",
    "    story_truth = extract_search_term_regex(story, \n",
    "                                            search_type = search_type, \n",
    "                                            return_context = True)\n",
    "    title_truth = extract_search_term_regex(title, \n",
    "                                            search_type = search_type, \n",
    "                                            return_context = True)\n",
    "    if type(story_truth) == str:\n",
    "        if type(title_truth) == str:\n",
    "            story_truth += title_truth\n",
    "        return story_truth\n",
    "    elif type(title_truth) == str:\n",
    "        return title_truth\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Returns a comma separated string of the features that match the search in question\n",
    "def extract_feature(story, feature = 'tx_type_search', title = None):\n",
    "    features = SEARCH_DICT[feature]\n",
    "    if pd.isnull(title):\n",
    "        searches = [story]\n",
    "    else:\n",
    "        searches = [story, title]\n",
    "        \n",
    "    return_str = ''\n",
    "    for x in searches:\n",
    "        if type(x) == str:\n",
    "            x = x.lower()\n",
    "            for f in features:\n",
    "                if f in x:\n",
    "                    if len(return_str) == 0:\n",
    "                        return_str += f\n",
    "                    else:\n",
    "                        return_str += ', '\n",
    "                        return_str += f\n",
    "                        \n",
    "    if len(return_str) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return return_str\n",
    "\n",
    "\n",
    "def collapse_feature(mentions, feature_dict):\n",
    "    if type(mentions) == str:\n",
    "        temp_mentions = mentions.split(', ')\n",
    "        new_mentions = []\n",
    "        for t in temp_mentions:\n",
    "            new_mentions.append(feature_dict[t])\n",
    "        \n",
    "        new_mentions = np.unique(new_mentions)\n",
    "        \n",
    "        new_mentions = ', '.join(new_mentions)\n",
    "        return new_mentions\n",
    "\n",
    "\n",
    "    \n",
    "def assign_num_occurrences(mentions):\n",
    "    if type(mentions) == str:\n",
    "        if ',' in mentions:\n",
    "            new = mentions.split(',')\n",
    "            return len(new)\n",
    "        else:\n",
    "            if mentions != '':\n",
    "                return 1\n",
    "        \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Mine each text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for oop_type\n",
      "extracting oop_type\n",
      "collapsing oop_type\n",
      "searching for insurance_type\n",
      "extracting insurance_type\n",
      "collapsing insurance_type\n",
      "searching for tx_type\n",
      "extracting tx_type\n",
      "collapsing tx_type\n",
      "searching for cancer_type\n",
      "extracting cancer_type\n",
      "collapsing cancer_type\n",
      "searching for brave\n",
      "extracting brave\n",
      "searching for nice\n",
      "extracting nice\n",
      "searching for thank\n",
      "extracting thank\n",
      "searching for self_reliance\n",
      "extracting self_reliance\n",
      "searching for battle\n",
      "extracting battle\n"
     ]
    }
   ],
   "source": [
    "#Look for clinical/financial details\n",
    "recode_feats_to_search =  ['oop_type', 'insurance_type', 'tx_type', 'cancer_type']\n",
    "df['story_and_title'] = df['title'] + ' ' + df['story']\n",
    "for r in recode_feats_to_search:\n",
    "    new_col = r + '_is_mentioned'\n",
    "    print(f\"searching for {r}\")\n",
    "    df[new_col] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                        search_type = r),\n",
    "                                   axis = 1)\n",
    "    print(f\"extracting {r}\")\n",
    "    df[r] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                        search_type = r,\n",
    "                                                        return_context = True),\n",
    "                                   axis = 1)\n",
    "    recode = 'collapsed_' + r\n",
    "\n",
    "    feat_dict = create_dict(r)\n",
    "    print(f\"collapsing {r}\")\n",
    "    df[recode] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                        search_type = r,\n",
    "                                                        collapse_dict = feat_dict),\n",
    "                                   axis = 1)\n",
    "\n",
    "\n",
    "df['num_tx'] = df['collapsed_tx_type'].apply(assign_num_occurrences)\n",
    "df['num_oop'] = df['collapsed_oop_type'].apply(assign_num_occurrences)\n",
    "df['uninsured'] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                            search_type = 'insurance_type',\n",
    "                                                            find_uninsured = True),\n",
    "                                   axis = 1)\n",
    "\n",
    "#Look for mention of Covid\n",
    "df[\"covid_is_mentioned\"] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                        search_type = \"covid\"),\n",
    "                   axis = 1)\n",
    "\n",
    "#Look for worth indicators\n",
    "worth_indicators = ['brave', 'nice', 'thank', 'self_reliance', 'battle']\n",
    "\n",
    "for w in worth_indicators:\n",
    "    new_col = w + '_is_mentioned'\n",
    "    print(f\"searching for {w}\")\n",
    "    df[new_col] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                        search_type = w),\n",
    "                                   axis = 1)\n",
    "    print(f\"extracting {w}\")\n",
    "    df[w] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                        search_type = w,\n",
    "                                                        return_context = True),\n",
    "                                   axis = 1)\n",
    "save = True\n",
    "if save:\n",
    "    df.drop(columns=['story_and_title'],inplace=True)\n",
    "    df.to_csv(data_io.input_cleaned/'gfm'/'US_cancer_campaigns_2018_2021_with_fips_and_text_features.csv',\n",
    "              encoding='utf-8',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['url', 'last_donation_time', 'last_update_time', 'location_city',\n",
       "       'location_country', 'location_stateprefix', 'poster', 'story', 'title',\n",
       "       'raised_amnt', 'goal_amnt', 'currency', 'tag', 'num_donors',\n",
       "       'num_likes', 'num_shares', 'day', 'month', 'year', 'location_city_only',\n",
       "       'cancer_in_story', 'cancer_in_title', 'location_county',\n",
       "       'location_state_county_fip', 'location_county_fip',\n",
       "       'oop_type_is_mentioned', 'oop_type', 'collapsed_oop_type',\n",
       "       'insurance_type_is_mentioned', 'insurance_type',\n",
       "       'collapsed_insurance_type', 'tx_type_is_mentioned', 'tx_type',\n",
       "       'collapsed_tx_type', 'cancer_type_is_mentioned', 'cancer_type',\n",
       "       'collapsed_cancer_type', 'num_tx', 'num_oop', 'uninsured',\n",
       "       'brave_is_mentioned', 'brave', 'nice_is_mentioned', 'nice',\n",
       "       'thank_is_mentioned', 'thank', 'self_reliance_is_mentioned',\n",
       "       'self_reliance', 'battle_is_mentioned', 'battle',\n",
       "       'collapsed_cancer_type_is_mentioned', 'covid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of campaigns that mentioned COVID vs all campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8729, (85531, 52))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.covid.sum(),df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfm",
   "language": "python",
   "name": "gfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
