{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import data_io\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14254\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>location</th>\n",
       "      <th>county_name</th>\n",
       "      <th>state</th>\n",
       "      <th>state_fips</th>\n",
       "      <th>county_fips</th>\n",
       "      <th>state_county_fips_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>el dorado, ks</td>\n",
       "      <td>Butler</td>\n",
       "      <td>KS</td>\n",
       "      <td>20</td>\n",
       "      <td>15</td>\n",
       "      <td>20015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>new rochelle, ny</td>\n",
       "      <td>Westchester</td>\n",
       "      <td>NY</td>\n",
       "      <td>36</td>\n",
       "      <td>119</td>\n",
       "      <td>36119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>columbia, pa</td>\n",
       "      <td>Lancaster</td>\n",
       "      <td>PA</td>\n",
       "      <td>42</td>\n",
       "      <td>71</td>\n",
       "      <td>42071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>forest hill, md</td>\n",
       "      <td>Harford</td>\n",
       "      <td>MD</td>\n",
       "      <td>24</td>\n",
       "      <td>25</td>\n",
       "      <td>24025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mount dora, fl</td>\n",
       "      <td>Lake</td>\n",
       "      <td>FL</td>\n",
       "      <td>12</td>\n",
       "      <td>69</td>\n",
       "      <td>12069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           location  county_name state  state_fips  county_fips  \\\n",
       "0     el dorado, ks       Butler    KS          20           15   \n",
       "1  new rochelle, ny  Westchester    NY          36          119   \n",
       "2      columbia, pa    Lancaster    PA          42           71   \n",
       "3   forest hill, md      Harford    MD          24           25   \n",
       "4    mount dora, fl         Lake    FL          12           69   \n",
       "\n",
       "  state_county_fips_str  \n",
       "0                 20015  \n",
       "1                 36119  \n",
       "2                 42071  \n",
       "3                 24025  \n",
       "4                 12069  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_locations = pd.read_csv(data_io.input_cleaned/'geolocations'/'unique_locations_w_fips.csv',\n",
    "                                    encoding='utf-8',\n",
    "                              dtype={'state_county_fips_str':'str'})\n",
    "print(len(unique_locations))\n",
    "unique_locations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_io.input_cleaned/'gfm'/'US_cancer_campaigns_2018_2021.csv',index_col=[0],\n",
    "                 sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df = pd.read_csv(data_io.input_cleaned/'gfm'/'exclusion_tracker_rd_2.csv',\n",
    "                              index_col = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop locations that didn't geocode\n",
    "unique_locations = unique_locations.replace('nan',np.nan).replace('none',np.nan)\n",
    "unique_locations.dropna(subset=['county_name'], inplace=True)\n",
    "\n",
    "county_dict = dict(zip(unique_locations['location'].to_list(), unique_locations['county_name'].to_list()))\n",
    "fips_dict = dict(zip(unique_locations['location'].to_list(), unique_locations['county_fips'].to_list()))\n",
    "long_fips_dict = dict(zip(unique_locations['location'].to_list(), \n",
    "                          unique_locations['state_county_fips_str'].to_list()))\n",
    "\n",
    "\n",
    "cleaned_location_city = df['location_city'].str.lower().str.strip()\n",
    "df['location_county'] = cleaned_location_city.map(county_dict)\n",
    "df['location_county_fip'] = cleaned_location_city.map(fips_dict)\n",
    "df['location_state_county_fip'] = cleaned_location_city.map(long_fips_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2394, 3)\n"
     ]
    }
   ],
   "source": [
    "unique_locs_to_scrape = df.loc[df.location_state_county_fip.isna(),\n",
    "                               ['location_city','location_city_only','location_stateprefix']].drop_duplicates()\n",
    "print(unique_locs_to_scrape.shape)\n",
    "unique_locs_to_scrape.to_csv(data_io.input_cleaned/'geolocations'/'unique_locations_to_scrape.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df.loc['deleted', 'failed_geocode'] = df['county'].isnull().sum()\n",
    "df = df.dropna(subset = ['county'])\n",
    "exclusion_df.loc['total', 'failed_geocode'] = len(df)\n",
    "exclusion_df.to_csv(data_io.input_cleaned/'gfm'/'final_exclusion_tracker.csv')\n",
    "geo_fail = df[pd.isnull(df['county'])]\n",
    "#save failed geocodes to make sure nothing in the US failed\n",
    "save = False\n",
    "if save:\n",
    "    geo_fail.to_csv(data_io.input_cleaned/'gfm'/'master_failed_geocode.csv', encoding='utf-8-sig')\n",
    "df.dropna(subset=['county'], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define text mining functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#functions for additional text mining\n",
    "SEARCH_OPTIONS = pd.read_csv(data_io.gfm/'gfm'/'free_text_search_terms.csv')\n",
    "\n",
    "SEARCH_DICT = {'cancer_type': SEARCH_OPTIONS['cancer_type'].dropna().to_list(),\n",
    "              'insurance_type': SEARCH_OPTIONS['insurance_type'].dropna().to_list(),\n",
    "              'oop_type': SEARCH_OPTIONS['oop_type'].dropna().to_list(),\n",
    "              'tx_type':SEARCH_OPTIONS['tx_type'].dropna().to_list(),\n",
    "              'clin_trial':SEARCH_OPTIONS['clin_trial'].dropna().to_list(),\n",
    "              'complementary':SEARCH_OPTIONS['complementary'].dropna().to_list(),\n",
    "              'battle':SEARCH_OPTIONS['battle'].dropna().to_list(),\n",
    "              'self_reliance':SEARCH_OPTIONS['self_reliance'].dropna().to_list(),\n",
    "              'journey': SEARCH_OPTIONS['journey'].dropna().to_list(),\n",
    "              'thank': SEARCH_OPTIONS['thank'].dropna().to_list(),\n",
    "              'nice':SEARCH_OPTIONS['nice'].dropna().to_list(),\n",
    "              'brave':SEARCH_OPTIONS['brave'].dropna().to_list(),\n",
    "              'financial_distress': SEARCH_OPTIONS['financial_distress'].dropna().to_list()}\n",
    "\n",
    "    \n",
    "def create_dict(search_type):\n",
    "    key_col = 'collapsed_'+search_type\n",
    "    new = SEARCH_OPTIONS.dropna(subset=[search_type])\n",
    "    this_dict = pd.Series(new[key_col].values,index=new[search_type].values).to_dict()\n",
    "    \n",
    "    return this_dict\n",
    "\n",
    "INSURE_DICT = create_dict('insurance_type')\n",
    "#OOP_DICT = create_dict('oop_type')\n",
    "import string   \n",
    "#import regex as re\n",
    "def extract_search_term_regex(x, search_type = 'cancer_type', return_context = False,\n",
    "                             find_uninsured = False, collapse_dict = 'none'):\n",
    "    if type(x) == str:\n",
    "        x = x.lower()\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    search_terms = SEARCH_DICT[search_type]\n",
    "    #match only if char after match is a space or punctuation\n",
    "    if 'cancer' in search_type:\n",
    "        for s in search_terms:\n",
    "            smatch = re.search(s+'\\W', x)\n",
    "            if smatch:\n",
    "                if return_context == True:\n",
    "                    end_smatch = smatch.span()[1]\n",
    "                    new = x[smatch.span()[0]:]\n",
    "                    new = new[0: new.find('.')]\n",
    "                    return new\n",
    "\n",
    "                return(x[smatch.span()[0]:smatch.span()[1]])\n",
    "        return np.nan\n",
    "    else:\n",
    "        return_val = False\n",
    "        uninsure = False\n",
    "        mention = []\n",
    "        collapsed_mention = []\n",
    "        for s in search_terms:\n",
    "            smatch = re.search(s, x)\n",
    "            \n",
    "            if smatch:\n",
    "                if return_context == True:\n",
    "                    \n",
    "                    new = x[smatch.span()[0]:smatch.span()[1]]\n",
    "                    #print(new)\n",
    "                    mention.append(new)\n",
    "                \n",
    "                if find_uninsured == True:\n",
    "                    if INSURE_DICT[s] == 'uninsured' or INSURE_DICT[s] == 'underunisured':\n",
    "                        return True\n",
    "                else:\n",
    "                    return_val = True\n",
    "                \n",
    "                if type(collapse_dict) != str:\n",
    "                    collapsed_mention.append(collapse_dict[s])\n",
    "\n",
    "                return_val = True\n",
    "                \n",
    "        if len(mention) >= 1:\n",
    "            mention = ','.join(mention)\n",
    "        else:\n",
    "            mention = None\n",
    "            \n",
    "        if len(collapsed_mention) >= 1:\n",
    "            collapsed_mention = np.unique(np.asarray(collapsed_mention))\n",
    "            collapsed_mention = list(collapsed_mention)\n",
    "            collapsed_mention = ','.join\n",
    "        else:\n",
    "            collapsed_mention = None\n",
    "            \n",
    "        if type(collapse_dict)!= str:\n",
    "            return collapsed_mention\n",
    "        \n",
    "        return mention if return_context == True else return_val\n",
    "\n",
    "def search_story_and_title(story, title, search_type):\n",
    "    story_truth = extract_search_term_regex(story, search_type = search_type)\n",
    "    title_truth = extract_search_term_regex(title, search_type = search_type)\n",
    "    if title_truth == True or story_truth == True:\n",
    "        return True\n",
    "    elif title_truth == False and story_truth == False:\n",
    "        return False\n",
    "\n",
    "def get_all_mentions(story, title, search_type):\n",
    "    story_truth = extract_search_term_regex(story, \n",
    "                                            search_type = search_type, \n",
    "                                            return_context = True)\n",
    "    title_truth = extract_search_term_regex(title, \n",
    "                                            search_type = search_type, \n",
    "                                            return_context = True)\n",
    "    if type(story_truth) == str:\n",
    "        if type(title_truth) == str:\n",
    "            story_truth += title_truth\n",
    "        return story_truth\n",
    "    elif type(title_truth) == str:\n",
    "        return title_truth\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "#Returns a comma separated string of the features that match the search in question\n",
    "def extract_feature(story, feature = 'tx_type_search', title = None):\n",
    "    features = SEARCH_DICT[feature]\n",
    "    if pd.isnull(title):\n",
    "        searches = [story]\n",
    "    else:\n",
    "        searches = [story, title]\n",
    "        \n",
    "    return_str = ''\n",
    "    for x in searches:\n",
    "        if type(x) == str:\n",
    "            x = x.lower()\n",
    "            for f in features:\n",
    "                if f in x:\n",
    "                    if len(return_str) == 0:\n",
    "                        return_str += f\n",
    "                    else:\n",
    "                        return_str += ', '\n",
    "                        return_str += f\n",
    "                        \n",
    "    if len(return_str) == 0:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return return_str\n",
    "\n",
    "\n",
    "def collapse_feature(mentions, feature_dict):\n",
    "    if type(mentions) == str:\n",
    "        temp_mentions = mentions.split(', ')\n",
    "        new_mentions = []\n",
    "        for t in temp_mentions:\n",
    "            new_mentions.append(feature_dict[t])\n",
    "        \n",
    "        new_mentions = np.unique(new_mentions)\n",
    "        \n",
    "        new_mentions = ', '.join(new_mentions)\n",
    "        return new_mentions\n",
    "\n",
    "\n",
    "    \n",
    "def assign_num_occurrences(mentions):\n",
    "    if type(mentions) == str:\n",
    "        if ',' in mentions:\n",
    "            new = mentions.split(',')\n",
    "            return len(new)\n",
    "        else:\n",
    "            if mentions != '':\n",
    "                return 1\n",
    "        \n",
    "    else:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  Mine each text feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "searching for oop_type\n",
      "extracting oop_type\n",
      "collapsing oop_type\n",
      "searching for insurance_type\n",
      "extracting insurance_type\n",
      "collapsing insurance_type\n",
      "searching for tx_type\n",
      "extracting tx_type\n",
      "collapsing tx_type\n",
      "searching for cancer_type\n",
      "extracting cancer_type\n",
      "collapsing cancer_type\n",
      "searching for brave\n",
      "extracting brave\n",
      "searching for nice\n",
      "extracting nice\n",
      "searching for thank\n",
      "extracting thank\n",
      "searching for self_reliance\n",
      "extracting self_reliance\n",
      "searching for battle\n",
      "extracting battle\n"
     ]
    }
   ],
   "source": [
    "recode = True\n",
    "if recode:\n",
    "    #Look for clinical/financial details\n",
    "    recode_feats_to_search = ['oop_type', 'insurance_type', 'tx_type',\n",
    "                                 'cancer_type']\n",
    "    df['story_and_title'] = df['title'] + ' ' + df['story']\n",
    "    for r in recode_feats_to_search:\n",
    "        new_col = r + '_is_mentioned'\n",
    "        print(f\"searching for {r}\")\n",
    "        df[new_col] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                            search_type = r),\n",
    "                                       axis = 1)\n",
    "        print(f\"extracting {r}\")\n",
    "        df[r] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                            search_type = r,\n",
    "                                                            return_context = True),\n",
    "                                       axis = 1)\n",
    "        recode = 'collapsed_' + r\n",
    "        \n",
    "        feat_dict = create_dict(r)\n",
    "        print(f\"collapsing {r}\")\n",
    "        df[recode] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                            search_type = r,\n",
    "                                                            collapse_dict = feat_dict),\n",
    "                                       axis = 1)\n",
    "\n",
    "    \n",
    "    df['num_tx'] = df['collapsed_tx_type'].apply(assign_num_occurrences)\n",
    "    df['num_oop'] = df['collapsed_oop_type'].apply(assign_num_occurrences)\n",
    "    df['uninsured'] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                                search_type = 'insurance_type',\n",
    "                                                                find_uninsured = True),\n",
    "                                       axis = 1)\n",
    "    #Look for worth indicators\n",
    "    worth_indicators = ['brave', 'nice', 'thank', 'self_reliance', 'battle']\n",
    "    \n",
    "    for w in worth_indicators:\n",
    "        new_col = w + '_is_mentioned'\n",
    "        print(f\"searching for {w}\")\n",
    "        df[new_col] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                            search_type = w),\n",
    "                                       axis = 1)\n",
    "        print(f\"extracting {w}\")\n",
    "        df[w] = df.apply(lambda x: extract_search_term_regex(x['story_and_title'],\n",
    "                                                            search_type = w,\n",
    "                                                            return_context = True),\n",
    "                                       axis = 1)\n",
    "    save = True\n",
    "    if save:\n",
    "        df.drop(columns=['story_and_title'],inplace=True)\n",
    "        df.to_csv(data_io.input_cleaned/'gfm'/'US_cancer_campaigns_2018_2021_with_fips_and_text_features.csv',\n",
    "                  encoding='utf-8',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gfm",
   "language": "python",
   "name": "gfm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
