{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "\n",
    "import data_io\n",
    "import pandas as pd\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_list = [\"AK\",\"AL\",\"AR\",\"AZ\",\"CA\",\"CO\",\"CT\",\"DC\",\"DE\",\"FL\",\"GA\",\n",
    "              \"HI\",\"IA\",\"ID\", \"IL\",\"IN\",\"KS\",\"KY\",\"LA\",\"MA\",\"MD\",\"ME\",\n",
    "              \"MI\",\"MN\",\"MO\",\"MS\",\"MT\",\"NC\",\"ND\",\"NE\",\"NH\",\n",
    "              \"NJ\",\"NM\",\"NV\",\"NY\", \"OH\",\"OK\",\"OR\",\"PA\",\n",
    "              \"RI\",\"SC\",\"SD\",\"TN\",\"TX\",\"UT\",\"VA\",\"VT\",\"WA\",\"WI\",\"WV\",\"WY\"]\n",
    "\n",
    "MIN_YEAR = 2010\n",
    "MAX_YEAR = 2019\n",
    "EXCEL_OPTIONS = {'strings_to_urls': False,\n",
    "                'strings_to_formulas': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "states_abbr_to_name = {\n",
    "        'AK': 'Alaska',\n",
    "        'AL': 'Alabama',\n",
    "        'AR': 'Arkansas',\n",
    "        'AS': 'American Samoa',\n",
    "        'AZ': 'Arizona',\n",
    "        'CA': 'California',\n",
    "        'CO': 'Colorado',\n",
    "        'CT': 'Connecticut',\n",
    "        'DC': 'District of Columbia',\n",
    "        'DE': 'Delaware',\n",
    "        'FL': 'Florida',\n",
    "        'GA': 'Georgia',\n",
    "        'GU': 'Guam',\n",
    "        'HI': 'Hawaii',\n",
    "        'IA': 'Iowa',\n",
    "        'ID': 'Idaho',\n",
    "        'IL': 'Illinois',\n",
    "        'IN': 'Indiana',\n",
    "        'KS': 'Kansas',\n",
    "        'KY': 'Kentucky',\n",
    "        'LA': 'Louisiana',\n",
    "        'MA': 'Massachusetts',\n",
    "        'MD': 'Maryland',\n",
    "        'ME': 'Maine',\n",
    "        'MI': 'Michigan',\n",
    "        'MN': 'Minnesota',\n",
    "        'MO': 'Missouri',\n",
    "        'MP': 'Northern Mariana Islands',\n",
    "        'MS': 'Mississippi',\n",
    "        'MT': 'Montana',\n",
    "        'NA': 'National',\n",
    "        'NC': 'North Carolina',\n",
    "        'ND': 'North Dakota',\n",
    "        'NE': 'Nebraska',\n",
    "        'NH': 'New Hampshire',\n",
    "        'NJ': 'New Jersey',\n",
    "        'NM': 'New Mexico',\n",
    "        'NV': 'Nevada',\n",
    "        'NY': 'New York',\n",
    "        'OH': 'Ohio',\n",
    "        'OK': 'Oklahoma',\n",
    "        'OR': 'Oregon',\n",
    "        'PA': 'Pennsylvania',\n",
    "        'PR': 'Puerto Rico',\n",
    "        'RI': 'Rhode Island',\n",
    "        'SC': 'South Carolina',\n",
    "        'SD': 'South Dakota',\n",
    "        'TN': 'Tennessee',\n",
    "        'TX': 'Texas',\n",
    "        'UT': 'Utah',\n",
    "        'VA': 'Virginia',\n",
    "        'VI': 'Virgin Islands',\n",
    "        'VT': 'Vermont',\n",
    "        'WA': 'Washington',\n",
    "        'WI': 'Wisconsin',\n",
    "        'WV': 'West Virginia',\n",
    "        'WY': 'Wyoming'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isBlank (myString):\n",
    "    if myString and myString.strip():\n",
    "        #myString is not None AND myString is not empty or blank\n",
    "        return False\n",
    "    #myString is None OR myString is empty or blank\n",
    "    return True\n",
    "\n",
    "def remove_leading_whitespace(df, column):\n",
    "    new_series = pd.Series(len(df))\n",
    "    i = 0\n",
    "    for i in range(0, len(df)):\n",
    "        old_str = df.loc[i, column]\n",
    "        new_str = old_str.lstrip()\n",
    "        new_series[i] = new_str\n",
    "\n",
    "    return new_series\n",
    "\n",
    "def get_loc_string(test1):\n",
    "\n",
    "    test1 = test1.split('\\r\\n\\xa0 ')\n",
    "    test1 = test1[1]\n",
    "\n",
    "    i = 0\n",
    "    for i in range(0, len(test1) - 1):\n",
    "        if test1[i].isspace() and test1[i+1].isspace():\n",
    "            test1 = test1[0:i]\n",
    "            break\n",
    "    return test1\n",
    "\n",
    "def only_numerics(seq):\n",
    "    seq_type= type(seq)\n",
    "    return seq_type().join(filter(seq_type.isdigit, seq))\n",
    "\n",
    "def extract_year(x):\n",
    "    if type(x) != str:\n",
    "        return np.nan\n",
    "    else:\n",
    "        x = x.lower()\n",
    "    if x == 'none' or 'invalid date' in x or x == 'Created':\n",
    "        return np.nan\n",
    "    else:\n",
    "        #print(x[-4:])\n",
    "        try:\n",
    "            new_var = int(x[-4:])\n",
    "        except:\n",
    "            return np.nan\n",
    "        return new_var\n",
    "\n",
    "\n",
    "def extract_whole_date(x):\n",
    "    if type(x) != str or 'none' in x:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return x[8:]\n",
    "\n",
    "\n",
    "def get_social(x):\n",
    "    if type(x) != str:\n",
    "        return x\n",
    "    x = x.lower()\n",
    "    if 'none' in x or pd.isnull(x):\n",
    "        return np.nan\n",
    "    if ',' in x:\n",
    "        x = x.replace(',', '')\n",
    "    if ' shares' in x:\n",
    "        x = x.replace(' shares', '')\n",
    "    if 'total' in x:\n",
    "        x = x[0:x.find('total')]\n",
    "        \n",
    "    if ' share' in x:\n",
    "        x = x.replace(' share', '')\n",
    "    if ' followers' in x:\n",
    "        x = x.replace(' followers', '')\n",
    "    if ' follower' in x:\n",
    "        x = x.replace(' follower', '')\n",
    "    if 'k' in x:\n",
    "        new = x[0:x.find('k')]\n",
    "        if '.' in new:\n",
    "            trail = '00'\n",
    "            new = new.replace('.', '')\n",
    "        else:\n",
    "            trail = '000'\n",
    "            \n",
    "        return new+trail\n",
    "    if 'friend' in x or 'comment' in x:\n",
    "        return np.nan\n",
    "    return x\n",
    "\n",
    "\n",
    "\n",
    "def get_other_loc(x):\n",
    "    if type(x) != str or x == 'none':\n",
    "        return np.nan\n",
    "    if ',' in x:\n",
    "        split_str = x.split(',')\n",
    "        return split_str[0].lower()\n",
    "    else:\n",
    "        return x.lower()\n",
    "\n",
    "def get_state_var(x):\n",
    "    if x == 'none':\n",
    "        return np.nan\n",
    "\n",
    "    if type(x) == str:\n",
    "\n",
    "        if ',' in x:\n",
    "            new_list = x.split(',')\n",
    "            state_str = new_list[1]\n",
    "            if isBlank(state_str):\n",
    "                return np.nan\n",
    "            else:\n",
    "                return state_str.strip()\n",
    "        else:\n",
    "            return x[-2:]\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "    \n",
    "def extract_deprecated_val(x):\n",
    "    if 'k' in x:\n",
    "        x = x[0:x.find('k')]\n",
    "        if '.' in x:\n",
    "            trail = '00'\n",
    "            x = x.replace('.', '')\n",
    "        else:\n",
    "            trail = '000'\n",
    "\n",
    "    elif 'm' in x:\n",
    "        x = x[0:x.find('m')]\n",
    "        if '.' in x:\n",
    "            trail = '00000'\n",
    "            x = x.replace('.', '')\n",
    "        else:\n",
    "            trail = '000000'\n",
    "    elif 'b' in x:\n",
    "        x = x[0:x.find('b')]\n",
    "        if '.' in x:\n",
    "            trail = '00000000'\n",
    "            x = x.replace('.', '')\n",
    "        else:\n",
    "            trail = '000000000'\n",
    "    goal = x+trail\n",
    "\n",
    "    goal = int(only_numerics(goal))\n",
    "    return goal\n",
    "\n",
    "\n",
    "def get_money_raised(x):\n",
    "    if type(x) != str or 'none' in x:\n",
    "        return np.nan\n",
    "    elif '%' in x:\n",
    "        return np.nan\n",
    "    elif '$' not in x:\n",
    "        return 'NOT USD'\n",
    "    else:\n",
    "        x = x.lower()\n",
    "        if 'of' in x:\n",
    "            new_info = x.split('of')\n",
    "            try:\n",
    "                if 'k' in new_info[0] or 'm' in new_info[0]:\n",
    "                    money_raised = extract_deprecated_val(new_info[0])\n",
    "                else:\n",
    "                    money_raised = int(only_numerics(new_info[0]))\n",
    "            except:\n",
    "                print('failed to get money raised: ', x)\n",
    "                money_raised = np.nan\n",
    "\n",
    "        elif 'raised' in x:\n",
    "            if 'goal' in x:\n",
    "                new = x.split('\\n')\n",
    "                this_str = new[0]\n",
    "                this_str = this_str[this_str.find('$'):]\n",
    "                if '.' in this_str:\n",
    "                    new = this_str[0:this_str.find('.')]\n",
    "                    if 'k' in new or 'm' in new:\n",
    "                        money_raised = extract_deprecated_val(new)\n",
    "                    else:\n",
    "                        money_raised = int(only_numerics(new))\n",
    "            else:\n",
    "                try:\n",
    "                    if 'k' in x or 'm' in x:\n",
    "                        money_raised = extract_deprecated_val(x)\n",
    "                    else:\n",
    "                        money_raised = int(only_numerics(x))\n",
    "                except:\n",
    "                    print('failed to get money raised: ', x)\n",
    "                    money_raised = np.nan\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "\n",
    "        return money_raised\n",
    "\n",
    "    \n",
    "def get_goal(x):\n",
    "    if type(x) != str:\n",
    "        return np.nan\n",
    "    if '%' in x:\n",
    "        return np.nan\n",
    "    if '$' not in x:\n",
    "        return 'NOT USD'\n",
    "    x = x.lower()\n",
    "    if 'raised' in x and 'of' not in x:\n",
    "        if 'goal' in x:\n",
    "            new = x.split('\\n')\n",
    "            new = new[1]\n",
    "            new = new[new.find('$'):]\n",
    "            if 'k' in new or 'm' in new or 'b' in new:\n",
    "                goal = extract_deprecated_val(new)\n",
    "\n",
    "            else:\n",
    "                if '.' in x:\n",
    "                    new = new[0:new.find('.')]\n",
    "                goal = int(only_numerics(new))\n",
    "            return goal\n",
    "\n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        if 'of' in x:\n",
    "            new_info = x.split('of')\n",
    "            new = new_info[1]\n",
    "            if 'k' in new or 'm' in new or 'b' in new:\n",
    "                goal = extract_deprecated_val(new)\n",
    "            else:\n",
    "                if '.' in new:\n",
    "                    new = new[0:new.find('.')]\n",
    "                try:\n",
    "                    goal = int(only_numerics(new))\n",
    "                except:\n",
    "                    goal = 'failed'\n",
    "            return goal\n",
    "\n",
    "        elif 'goal' in x:\n",
    "            return int(only_numerics(x))\n",
    "        else:\n",
    "            print('failed to parse goal: ', x)\n",
    "            \n",
    "def get_num_contributors(x):\n",
    "\n",
    "    if type(x) == str and x != 'none':\n",
    "        x = x.lower()\n",
    "        if 'raised' in x and '$' not in x:\n",
    "            new = x.split('in')\n",
    "            if 'k' in new:\n",
    "                new = extract_deprecated_val(new)\n",
    "            else:\n",
    "                new = int(only_numerics(new[0]))\n",
    "            return new\n",
    "        elif 'donor' in x and 'day' not in x and 'month' not in x:\n",
    "            if 'k' in x:\n",
    "                new = extract_deprecated_val(x)\n",
    "            else:\n",
    "                new = int(only_numerics(x))\n",
    "            return new\n",
    "        elif 'people' in x or 'person' in x:\n",
    "            if 'by' in x:\n",
    "                str_split1 = x.split('by')\n",
    "                if 'in' in x:\n",
    "                    str_split2 = str_split1[1].split('in')\n",
    "                    new = str_split2[0]\n",
    "                    if 'k' in new:\n",
    "                        new = extract_deprecated_val(new)\n",
    "                    else:\n",
    "                        new = int(only_numerics(new))\n",
    "                    return new\n",
    "                else:\n",
    "                    new = str_split1[1]\n",
    "                    if 'k' in new:\n",
    "                        new = extract_deprecated_val(new)\n",
    "                    else:\n",
    "                        new = int(only_numerics(new))\n",
    "                    return new\n",
    "            else:\n",
    "                print(x)\n",
    "                return x\n",
    "            \n",
    "            \n",
    "        else:\n",
    "            return np.nan\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "def remove_non_loc_info(x):\n",
    "    if type(x) == str:\n",
    "        temp = x.lower()\n",
    "        if 'donations' in temp and ' 1 donation' not in temp:\n",
    "            #print('donations in x')\n",
    "            loc = temp.find('donations')\n",
    "            delete = loc+len('donations')\n",
    "            temp = temp[delete:]\n",
    "            #return new\n",
    "        if '1 donation' in temp:\n",
    "            #print('donation in x')\n",
    "            loc = temp.find('donation')\n",
    "            delete = loc+len('donation')\n",
    "            temp = temp[delete:]\n",
    "            #return new\n",
    "        if 'organizer' in temp:\n",
    "            #print('organizer in x')\n",
    "            loc = temp.find('organizer')\n",
    "            delete = loc+len('organizer')\n",
    "            temp = temp[delete:]\n",
    "\n",
    "        return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#regex cleaning functions\n",
    "import re \n",
    "def contruct_goal_pattern():\n",
    "    rtypes = [] # (type of values returned (raise,goal,both) , notation that money is recorded in (US vs foreign))\n",
    "    rpatterns = []\n",
    "    rpatterns.append(r'(.*)raised of(.*)goal')\n",
    "    rpatterns.append(r'(.*)of(.*)goal')\n",
    "    rpatterns.append(r'(.*)of(.*)')\n",
    "    rpatterns.append(r'(.*)raised of(.*)target')\n",
    "    rpatterns.append(r'Raised:(.*)Goal:(.*)')\n",
    "    rtypes+=[['both','US']] * 5\n",
    "    \n",
    "    rpatterns.append(r'(.*)des Ziels von(.*)') # german\n",
    "    rpatterns.append(\n",
    "        r'(.*)sur un objectif de(.*)') # french\n",
    "    rpatterns.append(r'(.*)del objetivo de(.*)') # spanish\n",
    "    rpatterns.append(r'(.*)da meta de(.*)') # romanian\n",
    "    rpatterns.append(r'(.*)su(.*)raccolti') # italian\n",
    "    rpatterns.append(r'(.*)van het doel(.*)') # dutch\n",
    "    rtypes+=[['both','foreign']] * 6 \n",
    "\n",
    "    rpatterns.append(r'(.*)raised')\n",
    "    rtypes+=[['raised','US']]\n",
    "    \n",
    "    rpatterns.append(r'(.*)réunis') # french\n",
    "    rpatterns.append(r'(.*)gesammelt') # german\n",
    "    rpatterns.append(r'(.*)recaudados') # spanish\n",
    "    rpatterns.append(r'(.*)arrecadados') # portugese\n",
    "    rpatterns.append(r'(.*)raccolti') # italian\n",
    "    rtypes+=[['raised','foreign']]*5\n",
    "\n",
    "    rpatterns.append(r'(.*)goal')\n",
    "    rpatterns.append(r'(.*)target')\n",
    "    rtypes+=[['goal','US']]*2 \n",
    "    \n",
    "    rpatterns.append(r'Objectif\\s*:(.*)') # french\n",
    "    rpatterns.append(r'Objetivo\\s*:(.*)') #spanish\n",
    "    rpatterns.append(r'(.*)Ziel') # german\n",
    "    rpatterns.append(r'Meta de(.*)') #romanian\n",
    "    rpatterns.append(r'(.*)obiettivo') # italian\n",
    "    rtypes+=[['goal','foreign']]*5 \n",
    "    patterns_collection = pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "    return patterns_collection\n",
    "\n",
    "\n",
    "GOAL_PATTERNS = contruct_goal_pattern()\n",
    "\n",
    "_clean_whitespace = lambda x: re.sub(r'\\s+', ' ', x).strip()\n",
    "\n",
    "THOUNDSAND_PATTERN = re.compile(r'\\d+[,.]*\\d*.*[k]')\n",
    "MILLION_PATTERN = re.compile(r'\\d+[,.]*\\d*.*[m]')\n",
    "BILLION_PATTERN = re.compile(r'\\d+[,.]*\\d*.*[b]')\n",
    "MONEY_PATTERN = re.compile(r\"\"\"( #start of group0, this is the desired output\n",
    "                                \\d+ #start digit of money amount, mustbe followed by abbr, number or marker, nonwords or end of string\n",
    "                                ((?<=\\d)[,.]\\d+)*  #(group1) this is an optional group that only appears if markers are present\n",
    "                                ((?<=\\d)[kmbKMB](?=\\W|$)){0,1} #(group2)match thousand,mill,bill abbreviation if present but only if theres one of them\n",
    "                                )#close group0\n",
    "                            \"\"\",re.VERBOSE)\n",
    "_remove_whitespace_inside_money = lambda x: re.sub(r'(?<=\\d|[,.])\\s(?=\\d|[,.]|[kmbKMB](?=\\W|$))','',x)\n",
    "_extract_money_amount = lambda x: MONEY_PATTERN.findall(_remove_whitespace_inside_money(x))\n",
    "def _switch_markers_to_us_notation(amnt):\n",
    "    chars = []\n",
    "    for c in amnt:\n",
    "        if c == ',':\n",
    "            chars.append('.')\n",
    "        elif c == '.':\n",
    "            chars.append(',')\n",
    "        else:\n",
    "            chars.append(c)\n",
    "    return ''.join(chars)\n",
    "\n",
    "def parse_money_into_floats(x,us_notation=True,switch_retry=True):\n",
    "    out = {'amount':np.nan,'currency':np.nan}\n",
    "    if pd.isnull(x): return out\n",
    "    old_x = x\n",
    "    x = x.strip().lower()\n",
    "    if len(x) == 0: return out\n",
    "    try:\n",
    "        amnt = _extract_money_amount(x)[0][0]\n",
    "        curr = x.replace(amnt,'').strip()\n",
    "        if not us_notation:\n",
    "            # money amount written in foreign notation\n",
    "            # need to swap , and . \n",
    "            amnt = _switch_markers_to_us_notation(amnt)\n",
    "        numeric_amnt = ''.join(re.findall('\\d*|[,.]*', amnt))\n",
    "        numeric_amnt = float(numeric_amnt.replace(',', ''))\n",
    "        trail = 1\n",
    "        if THOUNDSAND_PATTERN.search(amnt):\n",
    "            trail = 1000\n",
    "        elif MILLION_PATTERN.search(amnt):\n",
    "            trail = 1000000\n",
    "        elif BILLION_PATTERN.search(amnt):\n",
    "            trail = 1000000000\n",
    "        out['amount']=numeric_amnt * trail\n",
    "        out['currency'] = curr\n",
    "        return out\n",
    "    except:\n",
    "        if switch_retry:\n",
    "            print(f'[WARNING] failed to parse {old_x} but will retry by swapping , and .')\n",
    "            # ~ doesnt work, have to be not \n",
    "            out = parse_money_into_floats(x,us_notation=not us_notation,switch_retry=False)\n",
    "            if not pd.isna([*out.values()]).all():\n",
    "                print('[WARNING] parsed results might be inaccurate, check below')\n",
    "                print(f\"[RETRY OUTPUT] original:{x}|parsed_amnt:{out['amount']}|parsed_currency:{out['currency']}\")\n",
    "        else:\n",
    "            print(f'failed to parse original x:{old_x}|stripped:{x}')\n",
    "        return out\n",
    "    \n",
    "def get_raised_and_goal_amount(x, USD_only=True):\n",
    "    import re\n",
    "    out = {\"raised\": np.nan, \"goal\": np.nan,\"raised_amnt\":np.nan,\n",
    "           \"raised_curr\":np.nan,\"goal_amnt\":np.nan,\"goal_curr\":np.nan}\n",
    "    if x == 'none': return out\n",
    "    if USD_only:\n",
    "        if '$' not in x: return out\n",
    "    x = _clean_whitespace(x)\n",
    "    for rpattern, rtype in GOAL_PATTERNS.iteritems():\n",
    "        results = re.findall(rpattern, x)\n",
    "        if len(results) > 0:\n",
    "            results = results[0]  # pop out results\n",
    "            rtype_value,rtype_notation = rtype[0],rtype[1]\n",
    "            if rtype_value == 'both':\n",
    "                out[\"raised\"], out[\"goal\"] = results[0], results[1]\n",
    "                for k in [\"raised\",\"goal\"]:\n",
    "                    results = parse_money_into_floats(out[k],us_notation=rtype_notation=='US')\n",
    "                    out[k+\"_amnt\"],out[k+\"_curr\"] = results[\"amount\"],results[\"currency\"]\n",
    "            elif rtype_value == \"raised\":\n",
    "                out[\"raised\"] = results\n",
    "                results = parse_money_into_floats(out[\"raised\"],us_notation=rtype_notation=='US')\n",
    "                out[\"raised_amnt\"],out[\"raised_curr\"] = results[\"amount\"],results[\"currency\"]\n",
    "            elif rtype_value == \"goal\":\n",
    "                out[\"goal\"] = results\n",
    "                results = parse_money_into_floats(out[\"goal\"],us_notation=rtype_notation=='US')\n",
    "                out[\"goal_amnt\"],out[\"goal_curr\"] = results[\"amount\"],results[\"currency\"]\n",
    "            break\n",
    "    if pd.isna([*out.values()]).all(): print(f'failed to parse {x}')\n",
    "    return out\n",
    "\n",
    "\n",
    "\n",
    "def standardize_MBk_in_number_str(x):\n",
    "    if pd.isnull(x): return x\n",
    "    old_x = x\n",
    "    x = x.strip().lower()\n",
    "    if len(x) == 0: return np.nan\n",
    "    try:\n",
    "        x_i = re.findall('\\d+[,.]*\\d*', x)[0]\n",
    "        x_i = float(x_i.replace(',', ''))\n",
    "        trail = 1\n",
    "        if THOUNDSAND_PATTERN.search(x):\n",
    "            trail = 1000\n",
    "        elif MILLION_PATTERN.search(x):\n",
    "            trail = 1000000\n",
    "        elif BILLION_PATTERN.search(x):\n",
    "            trail = 1000000000\n",
    "        return x_i * trail\n",
    "    except:\n",
    "        print(f'original x:{old_x}|stripped:{x}')\n",
    "        return np.nan\n",
    "\n",
    "def contruct_date_pattern():\n",
    "    rpatterns = []\n",
    "    rtypes = []\n",
    "    rpatterns.append(r'Created ([a-zA-Z]+) (\\d+), (\\d+)')\n",
    "    rtypes.append(['month', 'day', 'year'])\n",
    "    rpatterns.append(r'Created (\\d+) ([a-zA-z]+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Created by .*?on ([a-zA-z]+) (\\d+), (\\d+)')\n",
    "    rtypes.append(['month', 'day', 'year'])\n",
    "    rpatterns.append(r'Erstellt am (\\d+). (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Date de création : (\\d+) (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Fecha de creación: (\\d+) de (\\S+) de (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Creata il (\\d+) (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Gemaakt op (\\d+) (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Criada em (\\d+) de (\\S+) de (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    # special case: put this in day field for later processing with archive_timestamp\n",
    "    # double parenthesis to match regex findall output as other patterns\n",
    "    rpatterns.append(r'Created ((\\d+ days ago))')\n",
    "    rtypes.append(['day', 'day'])\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "\n",
    "DATE_PATTERNS = contruct_date_pattern()\n",
    "\n",
    "def parse_created_date(x):\n",
    "    out = {\"day\": np.nan, \"month\": np.nan, \"year\": np.nan}\n",
    "    if x == 'none': return out\n",
    "    x = _clean_whitespace(x)\n",
    "    if x.find('Invalid date') > -1: return out\n",
    "    if x == 'Created': return out\n",
    "    for rpattern, rtype in DATE_PATTERNS.iteritems():\n",
    "        results = re.findall(rpattern, x)\n",
    "        if len(results) > 0:\n",
    "            results = results[0]  # pop out results\n",
    "            for k, v in zip(rtype, results):\n",
    "                out[k] = v\n",
    "            break\n",
    "    if pd.isna([*out.values()]).all(): print(f'failed to parse {x}')\n",
    "    return out\n",
    "\n",
    "def construct_status_pattern():\n",
    "    rpatterns = []\n",
    "    rtypes = []\n",
    "    rpatterns.append(r'^(\\S+) donor$') # ^ and $ help make match the whole string\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'raised by (\\S+) donor in \\S+? duration')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'\\S+? raised by (\\S+) donor in \\S+? duration')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "    rpatterns.append(r'campaign created .*?duration ago')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'^recent donor [(](\\S+)[)]$')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'goal reached!')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'campaign ended')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'only \\S+? duration left to reach goal!')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'be the first to like this donor \\S+? duration ago')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'\\S+? donor likes this donor \\S+? duration ago')\n",
    "    rtypes.append([])\n",
    "\n",
    "    rpatterns.append(r'gesammelt von (\\S+) donore{0,1}n{0,1} in \\S+? tage{0,1}n{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'gesammelt von (\\S+) donore{0,1}n{0,1} in \\S+? monate{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'gesammelt von (\\S+) donore{0,1}n{0,1} in \\S+? stunde{0,1}n{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "    rpatterns.append(r'(\\S+) donornes ont fait un don en \\S+? mois{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'(\\S+) donorne a fait un don en \\S+? mois{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'(\\S+) donornes ont fait un don en \\S+? jours{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'(\\S+) donorne a fait un don en \\S+? jours{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "    rpatterns.append(r'recaudados de (\\S+) donoras en \\S+? mese{0,1}s{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'recaudados de (\\S+) donoras en \\S+? días{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "    rpatterns.append(r'recolectados de (\\S+) donoras{0,1} en \\S+? días{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'recolectados de (\\S+) donoras{0,1} en \\S+? mese{0,1}s{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'recolectados de (\\S+) donoras{0,1} en \\S+? horas{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "\n",
    "    rpatterns.append(r'donati da (\\S+) donor[ae] in \\S+? mesi')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'donati da (\\S+) donor[ae] in \\S+? ore')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'donati da (\\S+) donor[ae] in \\S+? giorni')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "    rpatterns.append(r'arrecadados por (\\S+) pessoas em \\S+? meses')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'arrecadados por (\\S+) pessoas em \\S+? dias')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "\n",
    "    rpatterns.append(r'not launched yet!')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'campagne créée depuis \\S+? mois')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'kampagne vor \\S+? monate erstellt')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'campagna creata \\S+? giorni fa')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'la campaña se creó hace \\S+? días')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'ingezameld door (\\S+) donoren binnen \\S+? maanden')\n",
    "    rtypes.append(['ndonor'])\n",
    "\n",
    "\n",
    "    rpatterns.append(r'la campaña se creó hace \\S+? mese{0,1}s{0,1}')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'kampagne vor \\S+? monate{0,1} erstellt')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'kampagne vor \\S+? tage{0,1}n{0,1} erstellt')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'campanha criada \\S+? dias atrás')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'ingezameld door (\\S+) donoren binnen \\S+? dagen')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'la campaña se creó hace \\S+? horas{0,1}')\n",
    "    rtypes.append([])\n",
    "    rpatterns.append(r'(\\S+) donorne a fait un don en \\S+? mois{0,1}')\n",
    "    rtypes.append(['ndonor'])\n",
    "    rpatterns.append(r'campagne créée depuis \\S+? jours{0,1}')\n",
    "    rtypes.append([])\n",
    "\n",
    "\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "\n",
    "\n",
    "\n",
    "STATUS_PATTERNS = construct_status_pattern()\n",
    "\n",
    "\n",
    "def parse_status(x):\n",
    "    out = {'ndonor': np.nan}\n",
    "    if x == 'none': return out\n",
    "    parsed = False\n",
    "    for rpattern, rtype in STATUS_PATTERNS.iteritems():\n",
    "        results = re.findall(rpattern, x)\n",
    "        if len(results) > 0:\n",
    "            for k, v in zip(rtype, results):\n",
    "                out[k] = v\n",
    "            parsed = True\n",
    "            break\n",
    "    if (not parsed) & pd.isna([*out.values()]).all():\n",
    "        print(f'failed to parse {x}')\n",
    "    return out\n",
    "\n",
    "# remove period at end, lowercase everything, clean whitespace characters\n",
    "_reformat_status = lambda x: _clean_whitespace(x[:-1].lower() if x[-1]=='.' else x.lower())\n",
    "# replace variation of units with standard words\n",
    "_duration_regex = re.compile(r'months*|days*|mins*|hours*')\n",
    "_donor_regex = re.compile(r'people|person|donors*|donations*|supporters*')\n",
    "_standardize_nouns = lambda x: _donor_regex.sub('donor',_duration_regex.sub('duration',x))\n",
    "# piece it all together\n",
    "standardize_status = lambda x: _standardize_nouns(_reformat_status(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_weird_title_type_pattern():\n",
    "    rpatterns=[]\n",
    "    rtypes=[]\n",
    "    rpatterns.append(r'^Page Not Found$')\n",
    "    rpatterns.append(r'^Unknown Error$')\n",
    "    rpatterns.append(r'^502 Bad Gateway$')\n",
    "    rpatterns.append(r'^404 Not Found$')\n",
    "    rpatterns.append(r'^403 Forbidden$')\n",
    "    rtypes += ['error']*5\n",
    "\n",
    "    rpatterns.append(r'^none$')\n",
    "    rtypes+= ['missing']\n",
    "\n",
    "    # gfm logistic\n",
    "    rpatterns.append(r'- Local Widget Builder$')\n",
    "    rpatterns.append(r'(.*)GoFundMe Support$')\n",
    "    rtypes += ['logistic']*2\n",
    "\n",
    "    # general home pages\n",
    "    rpatterns.append(r'^GoFundMe, le 1er site de crowdfunding pour créer une cagnotte en ligne$')\n",
    "    rpatterns.append(r'^GoFundMe : la plateforme gratuite n°1 de la collecte de fonds$')\n",
    "    rpatterns.append(r'^GoFundMe, le site n°1 de financement participatif et de collecte de fonds en ligne sans frais de plateforme$')\n",
    "    rpatterns.append(r'^Donate Online [|] Make Online Donations to People You Know!$')\n",
    "    rpatterns.append(r'^GoFundMe: Top-Website für Crowdfunding und Fundraising$')\n",
    "    rpatterns.append(r'^GoFundMe – die weltgrößte Crowdfunding-Seite zum Spendensammeln$')\n",
    "    rpatterns.append(r'^Funding(.*)[|] Fundraising - GoFundMe$')\n",
    "    rpatterns.append(r'^Raise Money For (.*?)[|](.*?)Fundraising - GoFundMe$')\n",
    "    rpatterns.append(r'^Personal & Charity Online Fundraising Websites that WORK!$')\n",
    "    rpatterns.append(r'(.*?)Fundraising - Start a Free Fundraiser$')\n",
    "    rpatterns.append(r'^Fundraising für (.*?)[|] Sammle Geld für(.*?)[|] GoFundMe$')\n",
    "    rpatterns.append(r'^Top Crowdfunding-Seite zum Spendensammeln – GoFundMe$')\n",
    "    rpatterns.append(r'^Personal Online Fundraising Websites that Work[!]$')\n",
    "    rpatterns.append(r'^Raise Money for YOU!(.*)!')\n",
    "    rpatterns.append(r'^GoFundMe:(.*)1')\n",
    "    rpatterns.append(r'^Raise money for your(.*?)Ideas!$')\n",
    "    rpatterns.append(r'^Raise Money for(.*)[|] GoFundMe$')\n",
    "    rpatterns.append(r'^Fundraising Ideas for(.*)')\n",
    "    rpatterns.append(r'(.*)Fundraising [|] Raise Money for(.*)[|] GoFundMe$')\n",
    "    rpatterns.append(r'(.*)Fundraising: Raise Money for (.*)')\n",
    "    rpatterns.append(r'(.*)Fundraising [|] Crowdfunding for(.*)– Free at GoFundMe$')\n",
    "    rpatterns.append(r'^Fundraising Ideas for(.*)')\n",
    "    rpatterns.append(r'^Find success with these Creative Fundraising Idea$')\n",
    "    rpatterns.append(r'^(.*)Fundraising [|] Fundraiser - GoFundMe[!]$')\n",
    "    rtypes+=['homepage']*24\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "WEIRD_TITLE_TYPE_PATTERNS = construct_weird_title_type_pattern()\n",
    "\n",
    "def detect_weird_title_type(x):\n",
    "    out = {'type':np.nan}\n",
    "    for rpattern, rtype in WEIRD_TITLE_TYPE_PATTERNS.iteritems():\n",
    "        if re.search(rpattern,x):\n",
    "            out['type'] = rtype\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def construct_title_pattern():\n",
    "    rpatterns =[]\n",
    "    rtypes=[]\n",
    "    rpatterns.append(r'^Fundraiser by(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'(.*)by(.*)- GoFundMe$')\n",
    "    rtypes.append(['campaign_title','organizer'])\n",
    "    rpatterns.append(r'^Fundraiser for(.*?)by(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Collecte de fonds pour(.*?)organisée par(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Spendenkampagne von(.*?)für(.*?):(.*)')\n",
    "    rtypes.append(['organizer','benefiter','campaign_title'])\n",
    "    rpatterns.append(r'^Campanha de arrecadação de fundos para(.*?)por(.*?):(.*)')\n",
    "    rtypes.append(['organizer','benefiter','campaign_title'])\n",
    "    rpatterns.append(r'^Campaña de(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Campanha de arrecadação de fundos de (.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Cagnotte organisée par(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Spendenkampagne von(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Collecte de fonds organisée par(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Inzamelingsactie van(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Raccolta fondi di(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Cagnotte pour(.*?)organisée par(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Inzamelingsactie voor(.*?)van(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "TITLE_PATTERNS = construct_title_pattern()\n",
    "\n",
    "_remove_newline = lambda x: ' '.join(x.split()).strip()\n",
    "def parse_title(x):\n",
    "    out = {'benefiter': np.nan,'organizer':np.nan,'campaign_title':np.nan,'campaign_title_type':np.nan}\n",
    "    if x == 'none': return out\n",
    "    parsed = False\n",
    "    x = _remove_newline(x)\n",
    "    out['campaign_title_type'] = detect_weird_title_type(x)['type']\n",
    "    if not pd.isnull(out['campaign_title_type']): \n",
    "        return out\n",
    "    else:\n",
    "        out['campaign_title_type'] = 'campaign'\n",
    "    for rpattern, rtype in TITLE_PATTERNS.iteritems():\n",
    "        results = re.findall(rpattern, x)\n",
    "        if len(results) > 0:\n",
    "            results=results[0]\n",
    "            for k, v in zip(rtype, results):\n",
    "                out[k] = v.strip()\n",
    "            parsed = True\n",
    "            break\n",
    "    if not parsed:\n",
    "        out['campaign_title'] = x\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Define validation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def state_is_valid(x):\n",
    "    if type(x) != str:\n",
    "        return False\n",
    "    if x not in state_list:\n",
    "        return False\n",
    "    else:\n",
    "        return True\n",
    "#searches for determinig whether campaign mentions cancer\n",
    "SEARCHES = pd.read_csv(data_io.input_cleaned/'gfm'/'cancer_search_terms.csv', encoding='utf-8')\n",
    "CANCER_SEARCHES = SEARCHES['cancer_type'].to_list()\n",
    "\n",
    "\n",
    "def find_cancer_story_title(story, title):\n",
    "    \n",
    "    story_truth = True if type(story) == str else False\n",
    "    title_truth = True if type(title) == str else False\n",
    "\n",
    "    if story_truth == False and title_truth == False:\n",
    "        return False\n",
    "    else:\n",
    "        new_story = story.lower() if story_truth == True else 'bad'\n",
    "        new_title = title.lower() if title_truth == True else 'bad'\n",
    "        if any(i in new_story for i in CANCER_SEARCHES):\n",
    "            return True\n",
    "        if any(i in new_title for i in CANCER_SEARCHES):\n",
    "            return True\n",
    "        \n",
    "    return False\n",
    "\n",
    "def cancer_in_x(x):\n",
    "    if type(x) == str:\n",
    "        x = x.lower()\n",
    "        if any(i in x for i in CANCER_SEARCHES):\n",
    "            return True\n",
    "    return False\n",
    "    \n",
    "def immunotherapy_mention(x):\n",
    "    if type(x) == str:\n",
    "        if 'immunotherap' in x or 'immuno therap' in x:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def is_english(x):\n",
    "    if type(x) == str:\n",
    "        x = x.lower()\n",
    "        if 'des ziels' in x:\n",
    "            return False\n",
    "        elif 'gesammelt' in x:\n",
    "            return False\n",
    "        elif 'recolectados' in x:\n",
    "            return False\n",
    "        elif 'del objetivo' in x:\n",
    "            return False\n",
    "        elif 'da meta de' in x:\n",
    "            return False\n",
    "        else:\n",
    "            return True\n",
    "        \n",
    "        \n",
    "def tag_is_valid(x):\n",
    "    tag = x.lower()\n",
    "    if 'medical' in tag:\n",
    "        return True\n",
    "    elif 'emergenc' in tag:\n",
    "        return True\n",
    "    elif 'family' in tag:\n",
    "        return True\n",
    "    elif 'community' in tag:\n",
    "        return True\n",
    "    elif 'other' in tag:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def assign_new_tag(x):\n",
    "    tag = x.lower()\n",
    "    if 'medical' in tag:\n",
    "        return 'medical'\n",
    "    else:\n",
    "        return np.nan\n",
    "    \n",
    "\n",
    "def medical_in_tag(x):\n",
    "    if type(x) == str:\n",
    "        if 'medical' in x:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    return False\n",
    "\n",
    "def check_for_spain(x, currency):\n",
    "    if type(currency)== str:\n",
    "        if currency == 'NOT USD':\n",
    "            if type(x) == str:\n",
    "                x = x.lower()\n",
    "                if 'ct, spain' in x or ('barcelona' in x and 'spain' in x):\n",
    "                    return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Read in master dataframe and set up exclusion tracker\n",
    "\n",
    "Master dataframe is generated by Make master tables.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DF = pd.read_csv(data_io.input_raw/'gfm'/'all_output_no_duplicate.csv', \n",
    "                        encoding='utf-8', sep='|', index_col=[0], dtype=str)\n",
    "original_length = len(MASTER_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MASTER_DF.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make a copy to be double sure you don't change master\n",
    "df = MASTER_DF.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Parse variables using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tqdm=True\n",
    "if use_tqdm: from tqdm import trange, tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse out title for benefiter, organizer, and campaign_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tqdm: \n",
    "    tqdm.pandas(desc='Parsing title')\n",
    "    title_parsed_dicts=df.title.progress_apply(parse_title)\n",
    "else:\n",
    "    title_parsed_dicts=df.title.apply(parse_title)\n",
    "title_parsed_df=pd.DataFrame.from_records(title_parsed_dicts,index=title_parsed_dicts.index)\n",
    "title_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join parsed results into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_cols = ['benefiter','organizer','campaign_title','campaign_title_type']\n",
    "df.drop(columns=parsed_cols,errors='ignore',inplace=True)\n",
    "df = df.merge(title_parsed_df[parsed_cols],on='campaign_id',how='left',indicator=True)\n",
    "# See merge results\n",
    "print(df._merge.value_counts())\n",
    "df.drop(columns='_merge',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Review parsed results by diff campaign_title_type\n",
    "df.loc[:,['title','benefiter','organizer','campaign_title','campaign_title_type']].groupby('campaign_title_type').head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse day,month & year from created_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tqdm: \n",
    "    tqdm.pandas(desc='Parsing date')\n",
    "    date_parsed_dicts = df.created_date.progress_apply(parse_created_date)\n",
    "else:\n",
    "    date_parsed_dicts = df.created_date.apply(parse_created_date)\n",
    "date_parsed_df = pd.DataFrame.from_records(date_parsed_dicts,index=date_parsed_dicts.index)                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some created_date have values \"Create X days ago\" and was parsed into the \"day\" field, fix this using archive_timestamp. these values are usually for campaign in 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the \"days ago rows\"\n",
    "days_ago_m = date_parsed_df.day.str.find('days ago') > -1\n",
    "days_ago_rows = date_parsed_df[days_ago_m]\n",
    "# Parse day month year from archived timestamp \n",
    "archived_ts= pd.DataFrame.from_records(df.loc[days_ago_rows.index, 'archive_timestamp'].apply(\n",
    "    lambda x: {\n",
    "        'day': x[6:8],\n",
    "        'month': x[4:6],\n",
    "        'year': x[0:4]\n",
    "    }),index=days_ago_rows.index)\n",
    "archived_ts['day'] = archived_ts['day'].astype(int)-days_ago_rows.day.apply(lambda x: int(x.replace('days ago','')))\n",
    "archived_ts['day']= archived_ts['day'].astype(str)\n",
    "# Update day month year fields \n",
    "date_parsed_df.update(archived_ts,overwrite=True)\n",
    "date_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join parsed results into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_cols = ['day','month','year']\n",
    "df.drop(columns=parsed_cols,errors='ignore',inplace=True) #drop if exists\n",
    "df = df.merge(date_parsed_df[['day','month','year']],on='campaign_id',how='left',indicator=True)\n",
    "# See merge results\n",
    "print(df._merge.value_counts())\n",
    "df.drop(columns='_merge',inplace=True)\n",
    "df.loc[:,['created_date','day','month','year']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map non-English months to standard English months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map_dict = {'février':'february','octobre':'october','juli':'july','junho':'june','09':'september','abril':'april',\n",
    "'março':'march','mars':'march','februar':'february','januar':'january', 'avril':'april','juin':'june',\n",
    "'juillet':'july','augustus':'august','mai':'may','mai':'may','märz':'march','juni':'June',\n",
    "'settembre':'september','gennaio':'january','septiembre':'september','mayo':'may',\n",
    "'décembre':'december','nisan':'April','maggio':'may','febbraio':'february',\n",
    "'marzo':'march','janvier':'january','dezember':'december','novembro':'november',\n",
    "'febrero':'february','aprile':'april','maio':'may','novembre':'november','mei':'may',\n",
    "'septembre':'september','oktober':'october','junio':'june','enero':'january','februari':'february','januari':'january',\n",
    "'fevereiro':'february','noviembre':'november','giugno':'june','agosto':'august'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map values\n",
    "mapped = df.month.str.lower().map(month_map_dict).str.capitalize()\n",
    "# if value was mapped keep the value, else keep original \n",
    "df.month=mapped.where(mapped.notna(),df.month)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure non-English month was mapped correctly to english month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.month.unique() # only english months should show up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for duplicates by campaign_title, organizer and parsed dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb=df.groupby(['campaign_title','organizer','day','month','year']).size().reset_index()\n",
    "po_dup_groups = gb[gb[0]>1]\n",
    "po_dups=df.merge(po_dup_groups,on=['campaign_title','organizer','day','month','year']).sort_values(['campaign_title','organizer','day','month','year'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check why there's still duplicates even tho we already stratified by cleaned_title,created_date, and lowered-case location in Make_master_table.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_dups[['created_date','location','campaign_title','organizer','day','month','year']].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Duplicates happened because of different original location or created_date, replace 'Organizer' values in location with 'none' and we'll keep the best duplicate in step 5a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.location = df.location.replace('Organizer','none')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find if any is in a non-English language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "show_cols =['gfm_url','cleaned_title','created_date','location','campaign_title','organizer','day','month','year']\n",
    "merge_cols  = ['campaign_title','organizer','day','month','year']\n",
    "non_eng_m = (~po_dups.status.apply(is_english)) | (~po_dups.goal.apply(is_english))\n",
    "po_dups_non_english=df.loc[:,show_cols].merge(po_dups.loc[non_eng_m,merge_cols],on=merge_cols).sort_values(by=merge_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_dups_non_english.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse goal into raised and goal amount "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: will fail for values where money was mentioned but not specified if it was raised or goal (e.g. '$8,130') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tqdm:\n",
    "    tqdm.pandas(desc='Parsing goal')\n",
    "    goal_parsed_dicts = df.goal.progress_apply(get_raised_and_goal_amount,**dict(USD_only=False)) \n",
    "else:\n",
    "    goal_parsed_dicts = df.goal.apply(get_raised_and_goal_amount,**dict(USD_only=False)) \n",
    "# toogle USD_only to parse euro,pound or % also, else it'd return np.nan for these fields\n",
    "goal_parsed_df = pd.DataFrame.from_records(goal_parsed_dicts,index=goal_parsed_dicts.index)\n",
    "goal_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse string inside 'raised' and 'goal' into amount and currency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see locations of campaigns that are not USD to see if they're based in the US"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_usd_m = (goal_parsed_df.raised_curr.str.find('$')==-1) | (goal_parsed_df.goal_curr.str.find('$')==-1)\n",
    "# look at location of these campaigns\n",
    "not_usd_locs=df.loc[not_usd_m,'location'].str.lower().value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From manual checking, at least the first 180 locations are not but there are almost 20,000 locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_usd_locs.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do a code check using get_state_var() & state_is_valid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_US_state_m = not_usd_locs.index.to_series().apply(get_state_var).apply(state_is_valid)\n",
    "# show any potential US location that have non_USD values\n",
    "not_usd_locs[valid_US_state_m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on values such as '2.2k','3M','1.5B' to make sure they're parsed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([goal_parsed_df.loc[goal_parsed_df.goal.str.find('k')>-1,:].head(3),\n",
    "goal_parsed_df.loc[goal_parsed_df.goal.str.lower().str.find('m')>-1,:].head(3),\n",
    "goal_parsed_df.loc[goal_parsed_df.goal.str.lower().str.find('b')>-1,:].head(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join parsed results into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "parsed_goal_cols = ['raised_amnt','raised_curr','goal_amnt','goal_curr']\n",
    "df.drop(columns=parsed_goal_cols,errors='ignore',inplace=True) # drop if exist\n",
    "df = df.merge(goal_parsed_df[parsed_goal_cols],on='campaign_id',how='left',indicator=True)\n",
    "# See merge results\n",
    "print(df._merge.value_counts())\n",
    "df.drop(columns='_merge',inplace=True)\n",
    "df.loc[:,['goal','raised_amnt','raised_curr','goal_amnt','goal_curr']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure non-english goals got parsed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "goal_format_example = 'obiettivo'\n",
    "goal_f_m=df.goal.str.find(goal_format_example)>-1\n",
    "df.loc[goal_f_m,['goal','raised_amnt','goal_amnt','raised_curr','goal_curr']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse status to get number of contributors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tqdm:\n",
    "    tqdm.pandas(desc='Standardizing status')\n",
    "    s_status = df.status.progress_apply(standardize_status)\n",
    "    tqdm.pandas(desc='Parsing status')\n",
    "    status_parsed_dicts = s_status.progress_apply(parse_status)\n",
    "else:\n",
    "    s_status = df.status.apply(standardize_status)\n",
    "    status_parsed_dicts = s_status.apply(parse_status)\n",
    "status_parsed_df = pd.DataFrame.from_records(status_parsed_dicts,index=status_parsed_dicts.index)\n",
    "status_parsed_df['num_contributors'] = pd.to_numeric(status_parsed_df.ndonor.apply(standardize_MBk_in_number_str))\n",
    "status_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check on values such as '2.2k','3M','1.5B' to make sure they're parsed correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.concat([status_parsed_df.loc[status_parsed_df.ndonor.str.find('k')>-1,:].head(3),\n",
    "status_parsed_df.loc[status_parsed_df.ndonor.str.lower().str.find('m')>-1,:].head(3),\n",
    "status_parsed_df.loc[status_parsed_df.ndonor.str.lower().str.find('b')>-1,:].head(3)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join parsed results into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.merge(status_parsed_df[['num_contributors']],on='campaign_id',how='left',indicator=True)\n",
    "# See merge results\n",
    "print(df._merge.value_counts())\n",
    "df.drop(columns='_merge',inplace=True)\n",
    "df.loc[:,['status','num_contributors']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Clean variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coarse cleaning for location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state'] = df['location'].apply(get_state_var)\n",
    "df['old_loc_copy'] = df['location'].copy()\n",
    "df['location'] = df['location'].apply(remove_non_loc_info)\n",
    "df['other_loc'] = df['location'].apply(get_other_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean social media info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['likes'] = df['num_likes'].apply(get_social)\n",
    "df['shares'] = df['num_shares'].apply(get_social)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tag'] = df['tag'].str.lower()\n",
    "df['new_tag'] = df['tag'].apply(assign_new_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Drop duplicates based on parsed title and parsed date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep track of number of campaigns we will exclude"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df = pd.read_csv(data_io.input_cleaned/'gfm'/'exclusion_tracker_rd_1.csv',\n",
    "                          index_col = 0)\n",
    "\n",
    "\n",
    "#these numbers come from Make master table notebook\n",
    "exclusion_df.loc['total', 'original_campaign_count'] = 1856154\n",
    "exclusion_df.loc['deleted', 'original_campaign_count'] = 0\n",
    "\n",
    "exclusion_df.loc[0, 'original_campaign_num'] = 1856154\n",
    "exclusion_df.loc[1, 'original_campaign_num'] = 1856154\n",
    "\n",
    "exclusion_df.loc['total', 'duplicate_url'] = 1835822\n",
    "exclusion_df.loc['deleted', 'duplicate_url'] = (exclusion_df.loc['total', 'original_campaign_count'] - \n",
    "                                                exclusion_df.loc['total', 'duplicate_url'])\n",
    "\n",
    "exclusion_df.loc['total', 'poor_wayback_qual'] = 1809281\n",
    "exclusion_df.loc['deleted', 'poor_wayback_qual'] = (exclusion_df.loc['total', 'duplicate_url'] - \n",
    "                                                    exclusion_df.loc['total', 'poor_wayback_qual'])\n",
    "\n",
    "\n",
    "exclusion_df.loc['total', 'duplicate_title_organizer_date_loc'] = len(df)\n",
    "exclusion_df.loc['deleted', 'duplicate_title_organizer_date_loc'] = (exclusion_df.loc['total', 'poor_wayback_qual'] - \n",
    "                                                                    exclusion_df.loc['total', 'duplicate_title_organizer_date_loc'])\n",
    "\n",
    "exclusion_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to keep best duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_best_duplicate(df,subset=['title', 'location'],use_tqdm=False):\n",
    "    # For processing, these columns will be dropped later\n",
    "    df = df.assign(keep_this_duplicate=False,uid=range(df.shape[0]))\n",
    "    # Get potentially duplicated campaigns\n",
    "    mb_duplicates_m = df.duplicated(subset=subset, keep=False)\n",
    "    mb_duplicates = df.loc[mb_duplicates_m, :]\n",
    "    # Higher score means having this field != none is more important\n",
    "    importance_score = pd.Series({\n",
    "        'goal': 10,\n",
    "        'created_date': 10,\n",
    "        'status': 5,\n",
    "        'num_likes': 5,\n",
    "        'num_shares': 5,\n",
    "        'story': 3,\n",
    "        'location': 3\n",
    "    })\n",
    "\n",
    "    def get_index_of_best_duplicate(group):\n",
    "        group = group.copy().replace('none',np.nan)\n",
    "        # since we're edditing group, pandas will act all weird so need to copy\n",
    "        # Calculate parsing quality\n",
    "        \n",
    "        for idx, row in group.iterrows():\n",
    "            group.loc[idx, 'parsing_quality'] = (row[importance_score.index].notna() *\n",
    "                                                 importance_score).sum()\n",
    "        # Sort campaigns by timestamp and consequently quality\n",
    "        # More recent timestamp and higher quality will be the last row\n",
    "        # if any archive_timestamp is nan, just sort by quality\n",
    "        if group.archive_timestamp.isna().any():\n",
    "            return group.sort_values(by=['parsing_quality']).uid.iloc[-1]  # return uid of last row\n",
    "        else:\n",
    "            return group.sort_values(by=['archive_timestamp', 'parsing_quality'\n",
    "                                     ],na_position='first').uid.iloc[-1]  # return uid of last row\n",
    "    # Process each group of duplicate\n",
    "    if use_tqdm:\n",
    "        # use tqdm to make it pretty\n",
    "        tqdm.pandas(desc='Processing duplicates')\n",
    "        best_duplicate_uids = mb_duplicates.groupby(\n",
    "            subset).progress_apply(get_index_of_best_duplicate)\n",
    "    else:\n",
    "        best_duplicate_uids = mb_duplicates.groupby(\n",
    "            subset).apply(get_index_of_best_duplicate)\n",
    "\n",
    "    # Signal the duplicate to keep based on uid\n",
    "    df.loc[df.uid.isin(best_duplicate_uids), 'keep_this_duplicate'] = True\n",
    "    # Return rows that are not duplicates OR is the best duplicate\n",
    "    return df.loc[(df.keep_this_duplicate & mb_duplicates_m)\n",
    "                  | ~mb_duplicates_m, :].drop(\n",
    "                      columns=['keep_this_duplicate', 'uid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-check duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb=df.groupby(['campaign_title','organizer','day','month','year']).size().reset_index()\n",
    "po_dup_groups = gb[gb[0]>1]\n",
    "po_dups=df.merge(po_dup_groups,on=['campaign_title','organizer','day','month','year']).sort_values(['campaign_title','organizer','day','month','year'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_dups[['created_date','location','campaign_title','organizer','day','month','year']].head(20)\n",
    "temp = po_dups.drop_duplicates(subset=['campaign_title', 'organizer', 'day', 'month', 'year'])\n",
    "print(len(temp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "old_df = df.copy()\n",
    "print(len(old_df))\n",
    "df = keep_best_duplicate(old_df, subset=['campaign_title','organizer', 'day','month', 'year'], use_tqdm=True)\n",
    "print(len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to make sure all duplicates were resolved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb=df.groupby(['campaign_title','organizer','day','month','year']).size().reset_index()\n",
    "po_dup_groups = gb[gb[0]>1]\n",
    "po_dups=df.merge(po_dup_groups,on=['campaign_title','organizer','day','month','year']).sort_values(['campaign_title','organizer','day','month','year'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "po_dups[['created_date','location','campaign_title','organizer','day','month','year']].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "exclusion_df.loc['total', 'duplicate_title_organizer_date'] = len(df)\n",
    "exclusion_df.loc['deleted', 'duplicate_title_organizer_date'] = (exclusion_df.loc['total', 'duplicate_title_organizer_date_loc'] -\n",
    "                                                                    exclusion_df.loc['total', 'duplicate_title_organizer_date'])\n",
    "print(exclusion_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save=False\n",
    "if save:\n",
    "    df.to_csv(data_io.input_cleaned/'gfm'/'all_campaigns.csv',sep='|',encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Exclude cases and update exclusion df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up columns for exclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cancer'] = df.apply(lambda x: find_cancer_story_title(x['story'], x['title']),axis=1)\n",
    "df['cancer_in_story'] = df['story'].apply(cancer_in_x)\n",
    "df['cancer_in_title'] = df['title'].apply(cancer_in_x)\n",
    "#df['immunotherapy'] = df['story'].apply(immunotherapy_mention)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df['cancer'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['state_in_us'] = df['state'].apply(state_is_valid)\n",
    "                                      \n",
    "df['status_is_english'] = df['status'].apply(is_english)\n",
    "df['money_is_english'] = df['goal'].apply(is_english)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##EXCLUDE BASED ON STATE\n",
    "df = df[df['state_in_us'] == True]\n",
    "print(len(df))\n",
    "exclusion_df.loc['deleted', 'state_not_us'] = (exclusion_df.loc['total', 'duplicate_title_organizer_date'] - \n",
    "                                              len(df))\n",
    "exclusion_df.loc['total', 'state_not_us'] = len(df)\n",
    "\n",
    "exclusion_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1776317 - 1528918"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exclude campaigns with goal not in USD since none would be from US (see test in step 4a)\n",
    "last_col = 'state_not_us'\n",
    "df = df.loc[df.goal.str.find('$')>-1,:]\n",
    "exclusion_df.loc['deleted', 'not_USD'] = exclusion_df.loc['total', last_col] - len(df)\n",
    "exclusion_df.loc['total', 'not_USD'] = len(df)\n",
    "\n",
    "print(len(df))\n",
    "print(exclusion_df.loc['total', last_col] - len(df))\n",
    "print(exclusion_df)\n",
    "last_col = 'not_USD'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make a copy as all US campaigns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['cancer']==True]\n",
    "exclusion_df.loc['deleted', 'not_cancer'] = exclusion_df.loc['total', last_col] - len(df)\n",
    "exclusion_df.loc['total', 'not_cancer'] = len(df)\n",
    "print(exclusion_df)\n",
    "print(len(df))\n",
    "print(exclusion_df.loc['total', last_col] - len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_col = 'not_cancer'\n",
    "#Exclude on tag\n",
    "df = df[pd.isnull(df['new_tag'])==False]\n",
    "exclusion_df.loc['deleted', 'tag_not_medical'] = exclusion_df.loc['total', last_col] - len(df)\n",
    "exclusion_df.loc['total', 'tag_not_medical'] = len(df)\n",
    "print(exclusion_df)\n",
    "print(len(df))\n",
    "print(exclusion_df.loc['total', last_col] - len(df))\n",
    "last_col = 'tag_not_medical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.year.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exclude on year if null\n",
    "\n",
    "df = df.dropna(subset=['year'])\n",
    "exclusion_df.loc['deleted', 'year_is_null'] = exclusion_df.loc['total', last_col] - len(df)\n",
    "exclusion_df.loc['total', 'year_is_null'] = len(df)\n",
    "print(exclusion_df)\n",
    "print(len(df))\n",
    "print(exclusion_df.loc['total', last_col] - len(df))\n",
    "exclusion_df.to_csv(data_io.input_cleaned/'gfm'/'exclusion_tracker_rd_2.csv')\n",
    "last_col = 'year_is_null'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = pd.ExcelWriter(data_io.input_cleaned/'gfm'/'cancer_campaigns_no_locs.xlsx',\n",
    "                        engine='xlsxwriter',\n",
    "                        options=EXCEL_OPTIONS)\n",
    "df.to_excel(writer, encoding='utf-8-sig')\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Generate unique location spreadsheet and merge with existing (if exists)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_if_canada(x):\n",
    "    #'ANA NAN'\n",
    "    if type(x) != str:\n",
    "        return False\n",
    "    if x[-2:] == 'ca':\n",
    "        if x[0].isalpha():\n",
    "            if x[1].isdigit():\n",
    "                if x[2].isalpha():\n",
    "                    if x[3].isdigit:\n",
    "                        if x[4].isalpha:\n",
    "                            if x[5].isdigit:\n",
    "                                return True\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NOTE: if none of the locations have been geocoded, you need to save this file and run it through the geocoder\n",
    "new_unique_locs = df.drop_duplicates(subset=['location'], keep='first')\n",
    "print(new_unique_locs['state_in_us'].value_counts())\n",
    "new_unique_locs = new_unique_locs[['location','state', 'other_loc']]\n",
    "new_unique_locs['possible_canada'] = new_unique_locs['location'].apply(check_if_canada)\n",
    "len(new_unique_locs)\n",
    "save = True\n",
    "if save:\n",
    "    writer = pd.ExcelWriter(data_io.input_cleaned/'geolocations'/'unique_locations_to_scrape_all_years.xlsx',\n",
    "                            engine='xlsxwriter',\n",
    "                            options={'strings_to_urls': False,\n",
    "                                     'strings_to_formulas': False})\n",
    "    new_unique_locs.to_excel(writer, encoding='utf-8-sig', index = False)\n",
    "    writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
