{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 19,
=======
   "execution_count": null,
>>>>>>> 5e206344ad9081178693706170e97a0a881a60e3
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src/')\n",
    "import data_io\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Merge the output tables into a giant csv "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: use tqdm to have progress bar of things that take long time by setting use_tqdm=True"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": null,
>>>>>>> 5e206344ad9081178693706170e97a0a881a60e3
   "metadata": {},
   "outputs": [],
   "source": [
    "use_tqdm=True\n",
    "if use_tqdm: from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. Only need to generate this once, can skip to part B if already have all_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fiter=(data_io.input_raw/'gfm').glob('*.csv')\n",
    "if use_tqdm: fiter = tqdm([*fiter])\n",
    "dflist = []\n",
    "for fp in fiter:\n",
    "    if use_tqdm: fiter.set_description(f\"Reading {fp.stem}\")\n",
    "    df = pd.read_csv(fp,encoding='utf-8',dtype=str)\n",
    "    dflist.append(df.assign(i_filename=fp.stem))\n",
    "\n",
    "dfs = pd.concat(dflist,ignore_index=True)\n",
    "dfs.fillna('none',inplace=True)\n",
    "# Save output\n",
    "dfs.to_csv(data_io.input_raw/'gfm'/'all_output.csv',index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. If ran part A at some point before, only need to read in all_output.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = pd.read_csv(data_io.input_raw/'gfm'/'all_output.csv',encoding='utf-8',index=False,sep=\"|\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Prepare for removing duplicates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define function that keeps the best duplicate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_best_duplicate(df,subset=['title', 'location'],use_tqdm=False):\n",
    "    # For processing, these columns will be dropped later\n",
    "    df = df.assign(keep_this_duplicate=False,uid=range(df.shape[0]))\n",
    "    # Get potentially duplicated campaigns\n",
    "    mb_duplicates_m = df.duplicated(subset=subset, keep=False)\n",
    "    mb_duplicates = df.loc[mb_duplicates_m, :]\n",
    "    # Higher score means having this field != none is more important\n",
    "    importance_score = pd.Series({\n",
    "        'goal': 10,\n",
    "        'created_date': 10,\n",
    "        'status': 5,\n",
    "        'num_likes': 5,\n",
    "        'num_shares': 5,\n",
    "        'story': 3,\n",
    "        'location': 3\n",
    "    })\n",
    "\n",
    "    def get_index_of_best_duplicate(group):\n",
    "        group = group.copy().replace('none',np.nan)\n",
    "        # since we're edditing group, pandas will act all weird so need to copy\n",
    "        # Calculate parsing quality\n",
    "        \n",
    "        for idx, row in group.iterrows():\n",
    "            group.loc[idx, 'parsing_quality'] = (row[importance_score.index].notna() *\n",
    "                                                 importance_score).sum()\n",
    "        # Sort campaigns by timestamp and consequently quality\n",
    "        # More recent timestamp and higher quality will be the last row\n",
    "        return group.sort_values(by=['archive_timestamp', 'parsing_quality'\n",
    "                                     ],na_position='first').uid.iloc[-1]  # return uid of last row\n",
    "    # Process each group of duplicate\n",
    "    if use_tqdm:\n",
    "        # use tqdm to make it pretty\n",
    "        tqdm.pandas(desc='Processing duplicates')\n",
    "        best_duplicate_uids = mb_duplicates.groupby(\n",
    "            subset).progress_apply(get_index_of_best_duplicate)\n",
    "    else:\n",
    "        best_duplicate_uids = mb_duplicates.groupby(\n",
    "            subset).apply(get_index_of_best_duplicate)\n",
    "\n",
    "    # Signal the duplicate to keep based on uid\n",
    "    df.loc[df.uid.isin(best_duplicate_uids), 'keep_this_duplicate'] = True\n",
    "    # Return rows that are not duplicates OR is the best duplicate\n",
    "    return df.loc[(df.keep_this_duplicate & mb_duplicates_m)\n",
    "                  | ~mb_duplicates_m, :].drop(\n",
    "                      columns=['keep_this_duplicate', 'uid'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.parse as up\n",
    "def clean_gfm_url(gurl):\n",
    "    cleaned_path = clean_path(up.urlsplit(gurl).path)\n",
    "    parts=['https','www.gofundme.com',cleaned_path,'','']\n",
    "    return up.urlunsplit(parts)\n",
    "def clean_path(path):\n",
    "    return '/'.join([p for p in path.split('/') if p!=''])\n",
    "def get_campaign_id(gurl):\n",
    "    return clean_path(up.urlsplit(gurl).path).split('/')[-1]\n",
    "def prepare_url_for_update(url):\n",
    "    # remove new gofundme /f/ redirect \n",
    "    return url.replace('gofundme.com/f/','gofundme.com/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create campaign_id column for each row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Clean gfm_url if value is not none, else left as none\n",
    "dfs.gfm_url = dfs.gfm_url.progress_apply(prepare_url_for_update)\n",
    "if use_tqdm:\n",
    "    tqdm.pandas(desc='Clean GFM url')\n",
    "    dfs.gfm_url = dfs.gfm_url.where(dfs.gfm_url=='none',other=dfs.gfm_url.progress_apply(clean_gfm_url))\n",
    "    tqdm.pandas(desc='Get campaign_id')\n",
    "    dfs = dfs.assign(campaign_id=dfs.gfm_url.progress_apply(get_campaign_id))\n",
    "else:\n",
    "    dfs.gfm_url = dfs.gfm_url.where(dfs.gfm_url=='none',other=dfs.gfm_url.apply(clean_gfm_url))\n",
    "    dfs = dfs.assign(campaign_id=dfs.gfm_url.apply(get_campaign_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove duplicates by campaign_id\n",
    "\n",
    "Also set up an \"exclusion df\" to track the number of campaigns excluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df = pd.DataFrame(columns=['original_campaign_count', 'duplicate_url', 'poor_wayback_qual',\n",
    "                                    'duplicate_title_organizer_date_loc', 'state_not_us', 'not_USD',\n",
    "                                    'not_cancer', 'tag_not_medical', 'year_is_null', 'failed_geocode',\n",
    "                                    'last_null_county_check'], index=['deleted', 'total'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_updated=dfs\n",
    "before_drop= dfs_updated.shape\n",
    "exclusion_df.loc['total', 'original_campaign_count'] = len(dfs)\n",
    "dfs_unique_gfmurl = keep_best_duplicate(dfs_updated,subset=['campaign_id'],use_tqdm=use_tqdm)\n",
    "exclusion_df.loc['total', 'duplicate_url'] = len(dfs_unique_gfmurl)\n",
    "exclusion_df.loc['deleted', 'duplicate_url'] = len(dfs) - len(dfs_unique_gfmurl)\n",
    "after_drop = dfs_unique_gfmurl.shape\n",
    "print(before_drop,after_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Confirm that we kept the right duplicate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "campaign_id = 't2n8g5n' # one of the example \n",
    "dfs_unique_gfmurl[dfs_unique_gfmurl.campaign_id=='t2n8g5n']\n",
    "# should return the more recent duplicate (wayback_status == 'present:success' one)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here on, we'll index the rows by their campaign_id since these should be unique now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_unique_gfmurl.set_index('campaign_id',inplace=True)\n",
    "dfs_unique_gfmurl.to_csv(data_io.input_raw/'gfm'/'all_output_unique_gfm_url.csv',encoding='utf-8',sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Remove rows that failed completely during scraping "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a sample of 5 campaigns for each type of wayback_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_unique_gfmurl['wayback_status_type'] = dfs_unique_gfmurl.wayback_status.apply(lambda x: ''.join([c for c in x if not c.isnumeric() ]))\n",
    "wayback_types = dfs_unique_gfmurl.groupby('wayback_status_type').head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types of wayback status there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wayback_status_type_counts=wayback_types.wayback_status_type.value_counts().sort_index(ascending=False)\n",
    "print(wayback_status_type_counts)\n",
    "all_wayback_status_types=wayback_status_type_counts.index.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To view example of a type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=5\n",
    "print(all_wayback_status_types[i])\n",
    "_m=wayback_types.wayback_status_type==all_wayback_status_types[i]\n",
    "wayback_types[_m].iloc[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a list of wayback_status_type that can be consider decently successful,\n",
    "we don't consider `wayback: scraped but did not meet success standard` as successful because usually these are redirects to gfm homepage not actual campaign webpage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allowed_wayback_status_types = ['present: success',\n",
    "                                'present: scraped but did not meet success criteria ; wayback: success',\n",
    "                                'present: scraped but did not meet success criteria ; wayback: inactive',\n",
    "                                'present: request failed ; wayback: success',\n",
    "                                'present: request failed ; wayback: inactive'\n",
    "                               ]\n",
    "allowed_wayback_status_types += ['present: inactive ; wayback: url not found in archives',\n",
    "                                'present: inactive ; wayback: success',\n",
    "                                'present: inactive ; wayback: scraped but did not meet success standard',\n",
    "                                'present: inactive ; wayback: request failed',\n",
    "                                'present: inactive ; wayback: no working archives out of  archives',\n",
    "                                'present: inactive ; wayback: inactive']\n",
    "allowed_wayback_status_types += ['present: campaign not found ; wayback: success',\n",
    "                                'present: campaign not found ; wayback: inactive']\n",
    "allowed_wayback_status_types += ['none'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_all_decent = dfs_unique_gfmurl.loc[dfs_unique_gfmurl.wayback_status_type.isin(allowed_wayback_status_types),:]\n",
    "exclusion_df.loc['total', 'poor_wayback_qual'] = len(dfs_all_decent)\n",
    "exclusion_df.loc['deleted', 'poor_wayback_qual'] = len(dfs_unique_gfmurl) - len(dfs_all_decent)\n",
    "print(dfs_unique_gfmurl.shape,dfs_all_decent.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Distribution of wayback status type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_all_decent.wayback_status_type.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the type of wayback_status_type that were excluded, make sure there's no type we still want to keep "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wayback_types.wayback_status_type[~wayback_types.wayback_status_type.isin(dfs_all_decent.wayback_status_type.unique())].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_all_decent.drop(columns=['wayback_status_type'],inplace=True)\n",
    "dfs_all_decent.to_csv(data_io.input_raw/'gfm'/'all_output_successful.csv',encoding='utf-8',sep='|')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Drop duplicates by title,organizer, date created and location"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def construct_weird_title_type_pattern():\n",
    "    rpatterns=[]\n",
    "    rtypes=[]\n",
    "    rpatterns.append(r'^Page Not Found$')\n",
    "    rpatterns.append(r'^Unknown Error$')\n",
    "    rpatterns.append(r'^502 Bad Gateway$')\n",
    "    rpatterns.append(r'^404 Not Found$')\n",
    "    rpatterns.append(r'^403 Forbidden$')\n",
    "    rtypes += ['error']*5\n",
    "\n",
    "    rpatterns.append(r'^none$')\n",
    "    rtypes+= ['missing']\n",
    "\n",
    "    # gfm logistic\n",
    "    rpatterns.append(r'- Local Widget Builder$')\n",
    "    rpatterns.append(r'(.*)GoFundMe Support$')\n",
    "    rtypes += ['logistic']*2\n",
    "\n",
    "    # general home pages\n",
    "    rpatterns.append(r'^GoFundMe, le 1er site de crowdfunding pour créer une cagnotte en ligne$')\n",
    "    rpatterns.append(r'^GoFundMe : la plateforme gratuite n°1 de la collecte de fonds$')\n",
    "    rpatterns.append(r'^GoFundMe, le site n°1 de financement participatif et de collecte de fonds en ligne sans frais de plateforme$')\n",
    "    rpatterns.append(r'^Donate Online [|] Make Online Donations to People You Know!$')\n",
    "    rpatterns.append(r'^GoFundMe: Top-Website für Crowdfunding und Fundraising$')\n",
    "    rpatterns.append(r'^GoFundMe – die weltgrößte Crowdfunding-Seite zum Spendensammeln$')\n",
    "    rpatterns.append(r'^Funding(.*)[|] Fundraising - GoFundMe$')\n",
    "    rpatterns.append(r'^Raise Money For (.*?)[|](.*?)Fundraising - GoFundMe$')\n",
    "    rpatterns.append(r'^Personal & Charity Online Fundraising Websites that WORK!$')\n",
    "    rpatterns.append(r'(.*?)Fundraising - Start a Free Fundraiser$')\n",
    "    rpatterns.append(r'^Fundraising für (.*?)[|] Sammle Geld für(.*?)[|] GoFundMe$')\n",
    "    rpatterns.append(r'^Top Crowdfunding-Seite zum Spendensammeln – GoFundMe$')\n",
    "    rpatterns.append(r'^Personal Online Fundraising Websites that Work[!]$')\n",
    "    rpatterns.append(r'^Raise Money for YOU!(.*)!')\n",
    "    rpatterns.append(r'^GoFundMe:(.*)1')\n",
    "    rpatterns.append(r'^Raise money for your(.*?)Ideas!$')\n",
    "    rpatterns.append(r'^Raise Money for(.*)[|] GoFundMe$')\n",
    "    rpatterns.append(r'^Fundraising Ideas for(.*)')\n",
    "    rpatterns.append(r'(.*)Fundraising [|] Raise Money for(.*)[|] GoFundMe$')\n",
    "    rpatterns.append(r'(.*)Fundraising: Raise Money for (.*)')\n",
    "    rpatterns.append(r'(.*)Fundraising [|] Crowdfunding for(.*)– Free at GoFundMe$')\n",
    "    rpatterns.append(r'^Fundraising Ideas for(.*)')\n",
    "    rpatterns.append(r'^Find success with these Creative Fundraising Idea$')\n",
    "    rpatterns.append(r'^(.*)Fundraising [|] Fundraiser - GoFundMe[!]$')\n",
    "    rtypes+=['homepage']*24\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "WEIRD_TITLE_TYPE_PATTERNS = construct_weird_title_type_pattern()\n",
    "\n",
    "def detect_weird_title_type(x):\n",
    "    out = {'type':np.nan}\n",
    "    for rpattern, rtype in WEIRD_TITLE_TYPE_PATTERNS.iteritems():\n",
    "        if re.search(rpattern,x):\n",
    "            out['type'] = rtype\n",
    "            break\n",
    "    return out\n",
    "\n",
    "def construct_title_pattern():\n",
    "    rpatterns =[]\n",
    "    rtypes=[]\n",
    "    rpatterns.append(r'^Fundraiser by(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'(.*)by(.*)- GoFundMe$')\n",
    "    rtypes.append(['campaign_title','organizer'])\n",
    "    rpatterns.append(r'^Fundraiser for(.*?)by(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Collecte de fonds pour(.*?)organisée par(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Spendenkampagne von(.*?)für(.*?):(.*)')\n",
    "    rtypes.append(['organizer','benefiter','campaign_title'])\n",
    "    rpatterns.append(r'^Campanha de arrecadação de fundos para(.*?)por(.*?):(.*)')\n",
    "    rtypes.append(['organizer','benefiter','campaign_title'])\n",
    "    rpatterns.append(r'^Campaña de(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Campanha de arrecadação de fundos de (.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Cagnotte organisée par(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Spendenkampagne von(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Collecte de fonds organisée par(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Inzamelingsactie van(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Raccolta fondi di(.*?):(.*)')\n",
    "    rtypes.append(['organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Cagnotte pour(.*?)organisée par(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    rpatterns.append(r'^Inzamelingsactie voor(.*?)van(.*?):(.*)')\n",
    "    rtypes.append(['benefiter','organizer','campaign_title'])\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "TITLE_PATTERNS = construct_title_pattern()\n",
    "\n",
    "_remove_newline = lambda x: ' '.join(x.split()).strip()\n",
    "def parse_title(x):\n",
    "    out = {'benefiter': np.nan,'organizer':np.nan,'campaign_title':np.nan,'campaign_title_type':np.nan}\n",
    "    if x == 'none': return out\n",
    "    parsed = False\n",
    "    x = _remove_newline(x)\n",
    "    out['campaign_title_type'] = detect_weird_title_type(x)['type']\n",
    "    if not pd.isnull(out['campaign_title_type']): \n",
    "        return out\n",
    "    else:\n",
    "        out['campaign_title_type'] = 'campaign'\n",
    "    for rpattern, rtype in TITLE_PATTERNS.iteritems():\n",
    "        results = re.findall(rpattern, x)\n",
    "        if len(results) > 0:\n",
    "            results=results[0]\n",
    "            for k, v in zip(rtype, results):\n",
    "                out[k] = v.strip()\n",
    "            parsed = True\n",
    "            break\n",
    "    if not parsed:\n",
    "        out['campaign_title'] = x\n",
    "    return out\n",
    "\n",
    "_clean_whitespace = lambda x: re.sub(r'\\s+', ' ', x).strip()\n",
    "\n",
    "def contruct_date_pattern():\n",
    "    rpatterns = []\n",
    "    rtypes = []\n",
    "    rpatterns.append(r'Created ([a-zA-Z]+) (\\d+), (\\d+)')\n",
    "    rtypes.append(['month', 'day', 'year'])\n",
    "    rpatterns.append(r'Created (\\d+) ([a-zA-z]+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Created by .*?on ([a-zA-z]+) (\\d+), (\\d+)')\n",
    "    rtypes.append(['month', 'day', 'year'])\n",
    "    rpatterns.append(r'Erstellt am (\\d+). (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Date de création : (\\d+) (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Fecha de creación: (\\d+) de (\\S+) de (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Creata il (\\d+) (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Gemaakt op (\\d+) (\\S+) (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    rpatterns.append(r'Criada em (\\d+) de (\\S+) de (\\d+)')\n",
    "    rtypes.append(['day', 'month', 'year'])\n",
    "    # special case: put this in day field for later processing with archive_timestamp\n",
    "    # double parenthesis to match regex findall output as other patterns\n",
    "    rpatterns.append(r'Created ((\\d+ days ago))')\n",
    "    rtypes.append(['day', 'day'])\n",
    "    return pd.Series(rtypes, index=rpatterns, name='rtype')\n",
    "\n",
    "DATE_PATTERNS = contruct_date_pattern()\n",
    "\n",
    "def parse_created_date(x):\n",
    "    out = {\"day\": np.nan, \"month\": np.nan, \"year\": np.nan}\n",
    "    if x == 'none': return out\n",
    "    x = _clean_whitespace(x)\n",
    "    if x.find('Invalid date') > -1: return out\n",
    "    if x == 'Created': return out\n",
    "    for rpattern, rtype in DATE_PATTERNS.iteritems():\n",
    "        results = re.findall(rpattern, x)\n",
    "        if len(results) > 0:\n",
    "            results = results[0]  # pop out results\n",
    "            for k, v in zip(rtype, results):\n",
    "                out[k] = v\n",
    "            break\n",
    "    if pd.isna([*out.values()]).all(): print(f'failed to parse {x}')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse out title for benefiter, organizer, and campaign_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "if use_tqdm: \n",
    "    tqdm.pandas(desc='Parsing title')\n",
    "    title_parsed_dicts=dfs_all_decent.title.progress_apply(parse_title)\n",
    "else:\n",
    "    title_parsed_dicts=dfs_all_decent.title.apply(parse_title)\n",
    "title_parsed_df=pd.DataFrame.from_records(title_parsed_dicts,index=title_parsed_dicts.index)\n",
    "title_parsed_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join parsed results into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_cols = ['benefiter','organizer','campaign_title','campaign_title_type']\n",
    "dfs_all_decent.drop(columns=parsed_cols,errors='ignore',inplace=True)\n",
    "dfs_all_decent = dfs_all_decent.merge(title_parsed_df[parsed_cols],right_index=True,left_index=True,how='left',indicator=True)\n",
    "# See merge results\n",
    "print(dfs_all_decent._merge.value_counts())\n",
    "dfs_all_decent.drop(columns='_merge',inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse day,month & year from created_date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if use_tqdm: \n",
    "    tqdm.pandas(desc='Parsing date')\n",
    "    date_parsed_dicts = dfs_all_decent.created_date.progress_apply(parse_created_date)\n",
    "else:\n",
    "    date_parsed_dicts = dfs_all_decent.created_date.apply(parse_created_date)\n",
    "date_parsed_df = pd.DataFrame.from_records(date_parsed_dicts,index=date_parsed_dicts.index)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fix the \"days ago\" issue in some rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix the \"days ago rows\"\n",
    "days_ago_m = date_parsed_df.day.str.find('days ago') > -1\n",
    "days_ago_rows = date_parsed_df[days_ago_m]\n",
    "print(date_parsed_df[days_ago_m].head())\n",
    "# Parse day month year from archived timestamp \n",
    "archived_ts= pd.DataFrame.from_records(dfs_all_decent.loc[days_ago_rows.index, 'archive_timestamp'].apply(\n",
    "    lambda x: {\n",
    "        'day': x[6:8],\n",
    "        'month': x[4:6],\n",
    "        'year': x[0:4]\n",
    "    }).values,index=days_ago_rows.index)\n",
    "archived_ts['day'] = archived_ts['day'].astype(int)-days_ago_rows.day.apply(lambda x: int(x.replace('days ago','')))\n",
    "archived_ts['day']= archived_ts['day'].astype(str)\n",
    "# Update day month year fields \n",
    "date_parsed_df.update(archived_ts,overwrite=True)\n",
    "print(date_parsed_df[days_ago_m].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Map non-English months to standard English months"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "month_map_dict = {'février':'february','octobre':'october','juli':'july','junho':'june','09':'september','abril':'april',\n",
    "'março':'march','mars':'march','februar':'february','januar':'january', 'avril':'april','juin':'june',\n",
    "'juillet':'july','augustus':'august','mai':'may','mai':'may','märz':'march','juni':'June',\n",
    "'settembre':'september','gennaio':'january','septiembre':'september','mayo':'may',\n",
    "'décembre':'december','nisan':'April','maggio':'may','febbraio':'february',\n",
    "'marzo':'march','janvier':'january','dezember':'december','novembro':'november',\n",
    "'febrero':'february','aprile':'april','maio':'may','novembre':'november','mei':'may',\n",
    "'septembre':'september','oktober':'october','junio':'june','enero':'january','februari':'february','januari':'january',\n",
    "'fevereiro':'february','noviembre':'november','giugno':'june','agosto':'august'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map values\n",
    "mapped = date_parsed_df.month.str.lower().map(month_map_dict).str.capitalize()\n",
    "# if value was mapped keep the value, else keep original \n",
    "print(date_parsed_df[mapped.notna()].head())\n",
    "date_parsed_df.month=mapped.where(mapped.notna(),date_parsed_df.month)\n",
    "print(date_parsed_df[mapped.notna()].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Join parsed results into df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_all_decent = dfs_all_decent.merge(date_parsed_df[['day','month','year']],right_index=True,left_index=True,how='left',indicator=True)\n",
    "# See merge results\n",
    "print(dfs_all_decent._merge.value_counts())\n",
    "dfs_all_decent.drop(columns='_merge',inplace=True)\n",
    "dfs_all_decent.loc[:,['created_date','day','month','year']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Keep best duplicate by campaign_title, organizer, date created, and location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dfs_all_decent = dfs_all_decent.assign(cleaned_location=dfs_all_decent.location.apply(lambda x: x.lower().strip()))\n",
    "dfs_unique = keep_best_duplicate(dfs_all_decent,subset=['campaign_title','organizer','day','month','year',\n",
    "                                                        'cleaned_location'],use_tqdm=True)\n",
    "exclusion_df.loc['total','duplicate_title_organizer_date_loc'] = len(dfs_unique)\n",
    "exclusion_df.loc['deleted', 'duplicate_title_organizer_date_loc'] = len(dfs_all_decent) - len(dfs_unique)\n",
    "print(dfs_all_decent.shape,dfs_unique.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exclusion_df.to_csv(data_io.input_cleaned/'gfm'/'exclusion_tracker_rd_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dfs_all_decent.shape,dfs_unique.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check to see if this is good criteria to identify duplicates, need to do this because sometimes the same campaign have different gfm_urls (when GFM move campaigns around or posters post twice) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_all_decent_gb = dfs_all_decent.groupby(['campaign_title','organizer','day','month','year','cleaned_location'])\n",
    "gb_size=dfs_all_decent_gb.size().sort_values(ascending=False)\n",
    "any_nonu = gb_size>1\n",
    "nonu_indexes =  any_nonu.index[any_nonu]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These duplicates should be the same campaign"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_size[any_nonu] # groups that are not unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dup=dfs_all_decent_gb.get_group(nonu_indexes[8]) # example of one group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_dup[['campaign_title','organizer','day','month','year','cleaned_location','story','goal','status']] \n",
    "# same campaign but diff campaign_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optional: sort by created_date and campaign_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_unique.sort_values(by=['year','campaign_id'],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_unique.drop(columns='cleaned_location',inplace=True)\n",
    "dfs_unique.to_csv(data_io.input_raw/'gfm'/'all_output_no_duplicate.csv',encoding='utf-8',sep='|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# review\n",
    "dfs_unique_in = pd.read_csv(data_io.input_raw/'gfm'/'all_output_no_duplicate.csv',\n",
    "                            encoding='utf-8',sep='|',dtype=str,index_col=[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.6.7"
=======
   "version": "3.6.8"
>>>>>>> 5e206344ad9081178693706170e97a0a881a60e3
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
